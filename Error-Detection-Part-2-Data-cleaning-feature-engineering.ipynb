{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly Data Science Immersive - Capstone Project #\n",
    "\n",
    "## Creating an automated English language error detector ##\n",
    "\n",
    "## Part 2: Data cleaning and feature engineering - overview of process\n",
    "\n",
    "This is the second part of my data science immersive Capstone Project, covering the data cleaning and feature engineering process. \n",
    "\n",
    "Recall that in part 1, I used the exam scripts in the FCE dataset to create overlapping ngrams of length 1 to 5 to query the Google Books Ngrams database. From this, I extracted \"match counts\" (frequency within the corpus) for each ngram, as well as match counts for the left and right context.\n",
    "\n",
    "In this phase, I combine these match counts and context counts with the FCE dataframe, create the features, check for errors / outliers, clean the data and then save as as new dataframes for EDA and modelling purposes in Part 3.\n",
    "\n",
    "**This entire process is completed twice - once for the training set and once for the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ErrorDetection as ed\n",
    "import importlib\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the FCE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FCE dataset and read in DataFrame\n",
    "my_file = \"fce_train.csv\"\n",
    "fce = pd.read_csv(my_file, index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have already remoed the null value sentence separators in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 452833 entries, 0 to 481562\n",
      "Data columns (total 2 columns):\n",
      "0    452833 non-null object\n",
      "1    452833 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "fce.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data created in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence indices, i.e. indices that mark out the sentences in our dataframe\n",
    "with open(\"sentence_indices_train.pickle\", 'rb') as f:\n",
    "    sentence_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dictionaries of ngram match counts\n",
    "with open('unigram_scores_col_train.pickle','rb') as f:\n",
    "    unigram_scores = pickle.load(f)\n",
    "with open('bigram_scores_col_train.pickle', 'rb') as f:\n",
    "    bigram_scores = pickle.load(f)\n",
    "with open('trigram_scores_col_train.pickle', 'rb') as f:\n",
    "    trigram_scores = pickle.load(f)\n",
    "with open('fourgram_scores_col_train.pickle', 'rb') as f:\n",
    "    fourgram_scores = pickle.load(f)\n",
    "with open('fivegram_scores_col_train.pickle', 'rb') as f:\n",
    "    fivegram_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dictionaries of ngram context scores\n",
    "with open('bigram_train_context.pickle','rb') as f:\n",
    "    bigram_context = pickle.load(f)\n",
    "with open('trigram_train_context.pickle', 'rb') as f:\n",
    "    trigram_context = pickle.load(f)\n",
    "with open('fourgram_train_context.pickle', 'rb') as f:\n",
    "    fourgram_context = pickle.load(f)\n",
    "with open('fivegram_train_context.pickle', 'rb') as f:\n",
    "    fivegram_context = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tagged ngrams\n",
    "with open('tagged_unigrams_train.pickle','rb') as f:\n",
    "    tagged_unigrams = pickle.load(f)\n",
    "with open('tagged_bigrams_train.pickle', 'rb') as f:\n",
    "    tagged_bigrams = pickle.load(f)\n",
    "with open('tagged_trigrams_train.pickle', 'rb') as f:\n",
    "    tagged_trigrams = pickle.load(f)\n",
    "with open('tagged_fourgrams_train.pickle', 'rb') as f:\n",
    "    tagged_fourgrams = pickle.load(f)\n",
    "with open('tagged_fivegrams_train.pickle', 'rb') as f:\n",
    "    tagged_fivegrams = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tagged ngram boundaries\n",
    "with open('tagged_unigrams_boundaries_train.pickle','rb') as f:\n",
    "    tagged_unigrams_boundaries = pickle.load(f)\n",
    "with open('tagged_bigrams_boundaries_train.pickle', 'rb') as f:\n",
    "    tagged_bigrams_boundaries = pickle.load(f)\n",
    "with open('tagged_trigrams_boundaries_train.pickle', 'rb') as f:\n",
    "    tagged_trigrams_boundaries = pickle.load(f)\n",
    "with open('tagged_fourgrams_boundaries_train.pickle', 'rb') as f:\n",
    "    tagged_fourgrams_boundaries = pickle.load(f)\n",
    "with open('tagged_fivegrams_boundaries_train.pickle', 'rb') as f:\n",
    "    tagged_fivegrams_boundaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create columns in the FCE dataset using ngram count dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a column for each ngram length\n",
    "for key in unigram_scores:\n",
    "    fce[key] = unigram_scores[key]\n",
    "for key in bigram_scores:\n",
    "    fce[key] = bigram_scores[key]\n",
    "for key in trigram_scores:\n",
    "    fce[key] = trigram_scores[key]\n",
    "for key in fourgram_scores:\n",
    "    fce[key] = fourgram_scores[key]\n",
    "for key in fivegram_scores:\n",
    "    fce[key] = fivegram_scores[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regard to the left and right context, to calculate probabilities I'm only interested in the first and last ngram for each ngram length respectively.\n",
    "\n",
    "This is easy to extract for our left contexts. However, if a word is at the beginning of a sentence (and therefore doesn't have a full complement of ngrams), the correct right context ngram will be mapped to the \"ngram_1_right_context\" key, otherwise it will be mapped to the \"ngram_2_right_context\" key.\n",
    "\n",
    "This will require an extra step of forward filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for each ngram left context score\n",
    "fce['bigram_left_context'] = bigram_context['bigram_1_left_context']\n",
    "fce['trigram_left_context'] = trigram_context['trigram_1_left_context']\n",
    "fce['fourgram_left_context'] = fourgram_context['fourgram_1_left_context']\n",
    "fce['fivegram_left_context'] = fivegram_context['fivegram_1_left_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "fce.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary dataframe with all right context columns\n",
    "fce_rc = pd.DataFrame()\n",
    "\n",
    "for key in bigram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = bigram_context[key]\n",
    "for key in trigram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = trigram_context[key]\n",
    "for key in fourgram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = fourgram_context[key]\n",
    "for key in fivegram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = fivegram_context[key]\n",
    "\n",
    "# replace \"no score\" with np.nan\n",
    "fce_rc.replace(\"No score\", np.nan, inplace=True)\n",
    "\n",
    "# forward fill\n",
    "fce_rc.loc[:, \"bigram_1_right_context\":\"bigram_2_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "fce_rc.loc[:, \"trigram_1_right_context\":\"trigram_3_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "fce_rc.loc[:, \"fourgram_1_right_context\":\"fourgram_4_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "fce_rc.loc[:, \"fivegram_1_right_context\":\"fivegram_5_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "\n",
    "# create new columns in main dataframe\n",
    "fce['bigram_right_context'] = fce_rc['bigram_2_right_context']\n",
    "fce['trigram_right_context'] = fce_rc['trigram_3_right_context']\n",
    "fce['fourgram_right_context'] = fce_rc['fourgram_4_right_context']\n",
    "fce['fivegram_right_context'] = fce_rc['fivegram_5_right_context']\n",
    "\n",
    "# delete temporary dataframe\n",
    "del fce_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 452833 entries, 0 to 481562\n",
      "Data columns (total 25 columns):\n",
      "0                         452833 non-null object\n",
      "1                         452833 non-null object\n",
      "unigram_1                 452833 non-null object\n",
      "bigram_1                  452833 non-null object\n",
      "bigram_2                  452833 non-null object\n",
      "trigram_1                 452833 non-null object\n",
      "trigram_2                 452833 non-null object\n",
      "trigram_3                 452833 non-null object\n",
      "fourgram_1                452833 non-null object\n",
      "fourgram_2                452833 non-null object\n",
      "fourgram_3                452833 non-null object\n",
      "fourgram_4                452833 non-null object\n",
      "fivegram_1                452833 non-null object\n",
      "fivegram_2                452833 non-null object\n",
      "fivegram_3                452833 non-null object\n",
      "fivegram_4                452833 non-null object\n",
      "fivegram_5                452833 non-null object\n",
      "bigram_left_context       452833 non-null object\n",
      "trigram_left_context      452833 non-null object\n",
      "fourgram_left_context     452833 non-null object\n",
      "fivegram_left_context     452833 non-null object\n",
      "bigram_right_context      425540 non-null object\n",
      "trigram_right_context     423518 non-null object\n",
      "fourgram_right_context    419524 non-null object\n",
      "fivegram_right_context    416502 non-null object\n",
      "dtypes: object(25)\n",
      "memory usage: 89.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# view created columns\n",
    "fce.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a separate dataframe with POS tags\n",
    "\n",
    "As I will be doing a different kind of processing of POS tages to match counts, I will keep the POS tags in a separate dataframe at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "fce_pos = fce.iloc[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tagged_unigrams:\n",
    "    fce_pos[key] = tagged_unigrams[key]\n",
    "for key in tagged_bigrams:\n",
    "    fce_pos[key] = tagged_bigrams[key]\n",
    "for key in tagged_trigrams:\n",
    "    fce_pos[key] = tagged_trigrams[key]\n",
    "for key in tagged_fourgrams:\n",
    "    fce_pos[key] = tagged_fourgrams[key]\n",
    "for key in tagged_fivegrams:\n",
    "    fce_pos[key] = tagged_fivegrams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "fce_pos[\"tagged_unigram_boundaries\"] = tagged_unigrams_boundaries\n",
    "fce_pos[\"tagged_bigram_boundaries\"] = tagged_bigrams_boundaries\n",
    "fce_pos[\"tagged_trigram_boundaries\"] = tagged_trigrams_boundaries\n",
    "fce_pos[\"tagged_fourgram_boundaries\"] = tagged_fourgrams_boundaries\n",
    "fce_pos[\"tagged_fivegram_boundaries\"] = tagged_fivegrams_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initial data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking an overview of the columns, it's clear that a number of issues will need to be resolved:\n",
    "- column names: some columns have a number not a name,\n",
    "- column types: our match count columns are objects, but I would expect these to be floats or integers (as they should contain only numerical values)\n",
    "- \"No score\" values: we will have to convert these to null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 452833 entries, 0 to 481562\n",
      "Data columns (total 25 columns):\n",
      "0                         452833 non-null object\n",
      "1                         452833 non-null object\n",
      "unigram_1                 452833 non-null object\n",
      "bigram_1                  452833 non-null object\n",
      "bigram_2                  452833 non-null object\n",
      "trigram_1                 452833 non-null object\n",
      "trigram_2                 452833 non-null object\n",
      "trigram_3                 452833 non-null object\n",
      "fourgram_1                452833 non-null object\n",
      "fourgram_2                452833 non-null object\n",
      "fourgram_3                452833 non-null object\n",
      "fourgram_4                452833 non-null object\n",
      "fivegram_1                452833 non-null object\n",
      "fivegram_2                452833 non-null object\n",
      "fivegram_3                452833 non-null object\n",
      "fivegram_4                452833 non-null object\n",
      "fivegram_5                452833 non-null object\n",
      "bigram_left_context       452833 non-null object\n",
      "trigram_left_context      452833 non-null object\n",
      "fourgram_left_context     452833 non-null object\n",
      "fivegram_left_context     452833 non-null object\n",
      "bigram_right_context      425540 non-null object\n",
      "trigram_right_context     423518 non-null object\n",
      "fourgram_right_context    419524 non-null object\n",
      "fivegram_right_context    416502 non-null object\n",
      "dtypes: object(25)\n",
      "memory usage: 89.8+ MB\n"
     ]
    }
   ],
   "source": [
    "fce.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>unigram_1</th>\n",
       "      <th>bigram_1</th>\n",
       "      <th>bigram_2</th>\n",
       "      <th>trigram_1</th>\n",
       "      <th>trigram_2</th>\n",
       "      <th>trigram_3</th>\n",
       "      <th>fourgram_1</th>\n",
       "      <th>fourgram_2</th>\n",
       "      <th>...</th>\n",
       "      <th>fivegram_4</th>\n",
       "      <th>fivegram_5</th>\n",
       "      <th>bigram_left_context</th>\n",
       "      <th>trigram_left_context</th>\n",
       "      <th>fourgram_left_context</th>\n",
       "      <th>fivegram_left_context</th>\n",
       "      <th>bigram_right_context</th>\n",
       "      <th>trigram_right_context</th>\n",
       "      <th>fourgram_right_context</th>\n",
       "      <th>fivegram_right_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear</td>\n",
       "      <td>c</td>\n",
       "      <td>25307860</td>\n",
       "      <td>1152194</td>\n",
       "      <td>No score</td>\n",
       "      <td>10042</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>8972</td>\n",
       "      <td>No score</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>25307860</td>\n",
       "      <td>1152194</td>\n",
       "      <td>10042</td>\n",
       "      <td>8972</td>\n",
       "      <td>44272603</td>\n",
       "      <td>18492</td>\n",
       "      <td>12343</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir</td>\n",
       "      <td>c</td>\n",
       "      <td>44272603</td>\n",
       "      <td>1152194</td>\n",
       "      <td>18492</td>\n",
       "      <td>10042</td>\n",
       "      <td>12343</td>\n",
       "      <td>No score</td>\n",
       "      <td>8972</td>\n",
       "      <td>2487</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>25307860</td>\n",
       "      <td>1152194</td>\n",
       "      <td>10042</td>\n",
       "      <td>8972</td>\n",
       "      <td>1371154850</td>\n",
       "      <td>20109</td>\n",
       "      <td>5128</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or</td>\n",
       "      <td>c</td>\n",
       "      <td>1371154850</td>\n",
       "      <td>18492</td>\n",
       "      <td>20109</td>\n",
       "      <td>10042</td>\n",
       "      <td>12343</td>\n",
       "      <td>5128</td>\n",
       "      <td>8972</td>\n",
       "      <td>2487</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>44272603</td>\n",
       "      <td>1152194</td>\n",
       "      <td>10042</td>\n",
       "      <td>8972</td>\n",
       "      <td>2652177</td>\n",
       "      <td>1303813</td>\n",
       "      <td>5128</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madam</td>\n",
       "      <td>c</td>\n",
       "      <td>2652177</td>\n",
       "      <td>20109</td>\n",
       "      <td>1303813</td>\n",
       "      <td>12343</td>\n",
       "      <td>5128</td>\n",
       "      <td>No score</td>\n",
       "      <td>8972</td>\n",
       "      <td>2487</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>1371154850</td>\n",
       "      <td>18492</td>\n",
       "      <td>10042</td>\n",
       "      <td>8972</td>\n",
       "      <td>22046138853</td>\n",
       "      <td>1303813</td>\n",
       "      <td>5128</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td>c</td>\n",
       "      <td>22046138853</td>\n",
       "      <td>1303813</td>\n",
       "      <td>No score</td>\n",
       "      <td>5128</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>2487</td>\n",
       "      <td>No score</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>2652177</td>\n",
       "      <td>20109</td>\n",
       "      <td>12343</td>\n",
       "      <td>8972</td>\n",
       "      <td>22046138853</td>\n",
       "      <td>1303813</td>\n",
       "      <td>5128</td>\n",
       "      <td>2487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I</td>\n",
       "      <td>c</td>\n",
       "      <td>1755464606</td>\n",
       "      <td>74006239</td>\n",
       "      <td>No score</td>\n",
       "      <td>314367</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>12886</td>\n",
       "      <td>No score</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>1755464606</td>\n",
       "      <td>74006239</td>\n",
       "      <td>314367</td>\n",
       "      <td>12886</td>\n",
       "      <td>41540111</td>\n",
       "      <td>1662948</td>\n",
       "      <td>12203</td>\n",
       "      <td>10977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>am</td>\n",
       "      <td>c</td>\n",
       "      <td>102062498</td>\n",
       "      <td>74006239</td>\n",
       "      <td>333632</td>\n",
       "      <td>314367</td>\n",
       "      <td>13805</td>\n",
       "      <td>No score</td>\n",
       "      <td>12886</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>1755464606</td>\n",
       "      <td>74006239</td>\n",
       "      <td>314367</td>\n",
       "      <td>12886</td>\n",
       "      <td>6051835146</td>\n",
       "      <td>52058492</td>\n",
       "      <td>42643038</td>\n",
       "      <td>73825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>writing</td>\n",
       "      <td>c</td>\n",
       "      <td>41540111</td>\n",
       "      <td>333632</td>\n",
       "      <td>1662948</td>\n",
       "      <td>314367</td>\n",
       "      <td>13805</td>\n",
       "      <td>12203</td>\n",
       "      <td>12886</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>No score</td>\n",
       "      <td>No score</td>\n",
       "      <td>102062498</td>\n",
       "      <td>74006239</td>\n",
       "      <td>314367</td>\n",
       "      <td>12886</td>\n",
       "      <td>123021129</td>\n",
       "      <td>44786667</td>\n",
       "      <td>74793</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>c</td>\n",
       "      <td>6051835146</td>\n",
       "      <td>1662948</td>\n",
       "      <td>52058492</td>\n",
       "      <td>13805</td>\n",
       "      <td>12203</td>\n",
       "      <td>42643038</td>\n",
       "      <td>12886</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>1062</td>\n",
       "      <td>No score</td>\n",
       "      <td>41540111</td>\n",
       "      <td>333632</td>\n",
       "      <td>314367</td>\n",
       "      <td>12886</td>\n",
       "      <td>7171979288</td>\n",
       "      <td>6584602</td>\n",
       "      <td>374649</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>order</td>\n",
       "      <td>c</td>\n",
       "      <td>123021129</td>\n",
       "      <td>52058492</td>\n",
       "      <td>44786667</td>\n",
       "      <td>12203</td>\n",
       "      <td>42643038</td>\n",
       "      <td>74793</td>\n",
       "      <td>144</td>\n",
       "      <td>10977</td>\n",
       "      <td>...</td>\n",
       "      <td>1062</td>\n",
       "      <td>0</td>\n",
       "      <td>6051835146</td>\n",
       "      <td>1662948</td>\n",
       "      <td>13805</td>\n",
       "      <td>12886</td>\n",
       "      <td>18384490</td>\n",
       "      <td>554113</td>\n",
       "      <td>812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1    unigram_1  bigram_1  bigram_2 trigram_1 trigram_2 trigram_3  \\\n",
       "0      Dear  c     25307860   1152194  No score     10042  No score  No score   \n",
       "1       Sir  c     44272603   1152194     18492     10042     12343  No score   \n",
       "2        or  c   1371154850     18492     20109     10042     12343      5128   \n",
       "3     Madam  c      2652177     20109   1303813     12343      5128  No score   \n",
       "4         ,  c  22046138853   1303813  No score      5128  No score  No score   \n",
       "6         I  c   1755464606  74006239  No score    314367  No score  No score   \n",
       "7        am  c    102062498  74006239    333632    314367     13805  No score   \n",
       "8   writing  c     41540111    333632   1662948    314367     13805     12203   \n",
       "9        in  c   6051835146   1662948  52058492     13805     12203  42643038   \n",
       "10    order  c    123021129  52058492  44786667     12203  42643038     74793   \n",
       "\n",
       "   fourgram_1 fourgram_2          ...           fivegram_4 fivegram_5  \\\n",
       "0        8972   No score          ...             No score   No score   \n",
       "1        8972       2487          ...             No score   No score   \n",
       "2        8972       2487          ...             No score   No score   \n",
       "3        8972       2487          ...             No score   No score   \n",
       "4        2487   No score          ...             No score   No score   \n",
       "6       12886   No score          ...             No score   No score   \n",
       "7       12886        144          ...             No score   No score   \n",
       "8       12886        144          ...             No score   No score   \n",
       "9       12886        144          ...                 1062   No score   \n",
       "10        144      10977          ...                 1062          0   \n",
       "\n",
       "   bigram_left_context trigram_left_context fourgram_left_context  \\\n",
       "0             25307860              1152194                 10042   \n",
       "1             25307860              1152194                 10042   \n",
       "2             44272603              1152194                 10042   \n",
       "3           1371154850                18492                 10042   \n",
       "4              2652177                20109                 12343   \n",
       "6           1755464606             74006239                314367   \n",
       "7           1755464606             74006239                314367   \n",
       "8            102062498             74006239                314367   \n",
       "9             41540111               333632                314367   \n",
       "10          6051835146              1662948                 13805   \n",
       "\n",
       "   fivegram_left_context bigram_right_context trigram_right_context  \\\n",
       "0                   8972             44272603                 18492   \n",
       "1                   8972           1371154850                 20109   \n",
       "2                   8972              2652177               1303813   \n",
       "3                   8972          22046138853               1303813   \n",
       "4                   8972          22046138853               1303813   \n",
       "6                  12886             41540111               1662948   \n",
       "7                  12886           6051835146              52058492   \n",
       "8                  12886            123021129              44786667   \n",
       "9                  12886           7171979288               6584602   \n",
       "10                 12886             18384490                554113   \n",
       "\n",
       "   fourgram_right_context fivegram_right_context  \n",
       "0                   12343                   2487  \n",
       "1                    5128                   2487  \n",
       "2                    5128                   2487  \n",
       "3                    5128                   2487  \n",
       "4                    5128                   2487  \n",
       "6                   12203                  10977  \n",
       "7                42643038                  73825  \n",
       "8                   74793                   1069  \n",
       "9                  374649                    313  \n",
       "10                    812                      0  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the first two columns\n",
    "fce.columns = [\"word\", \"y\"] + [col for col in fce.columns[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change our outcome variable to a binary variable\n",
    "fce.loc[:,\"y\"] = fce[\"y\"].map(lambda x: 0 if x == \"i\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with \"no score\" / null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"no score\" with null \n",
    "fce.replace(\"No score\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is one particular sentence where an error was returned from Phrasefinder and wasn't\n",
    "# picked up at an earlier stage. \n",
    "for col in fce.columns[2:]:\n",
    "    fce[col] = fce[col].map(lambda x: 0 if 'improve the disadvantages' \n",
    "                                    in str(x) else 22425 if 'the disadvantages' in str(x) else \n",
    "                                    2852366 if 'Dear Sir' in str(x) else 12139192 if 'Madame' in str(x)\n",
    "                                   else 0 if 'Sally' in str(x) else 256424470 if 'error' in str(x) else x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 452833 entries, 0 to 481562\n",
      "Data columns (total 25 columns):\n",
      "word                      452833 non-null object\n",
      "y                         452833 non-null int64\n",
      "unigram_1                 452833 non-null int64\n",
      "bigram_1                  452569 non-null float64\n",
      "bigram_2                  395635 non-null float64\n",
      "trigram_1                 450437 non-null float64\n",
      "trigram_2                 394216 non-null float64\n",
      "trigram_3                 342252 non-null float64\n",
      "fourgram_1                446180 non-null float64\n",
      "fourgram_2                392600 non-null float64\n",
      "fourgram_3                341465 non-null float64\n",
      "fourgram_4                292691 non-null float64\n",
      "fivegram_1                442948 non-null float64\n",
      "fivegram_2                390239 non-null float64\n",
      "fivegram_3                339869 non-null float64\n",
      "fivegram_4                291737 non-null float64\n",
      "fivegram_5                246467 non-null float64\n",
      "bigram_left_context       452569 non-null float64\n",
      "trigram_left_context      450437 non-null float64\n",
      "fourgram_left_context     446180 non-null float64\n",
      "fivegram_left_context     442948 non-null float64\n",
      "bigram_right_context      425540 non-null float64\n",
      "trigram_right_context     423518 non-null float64\n",
      "fourgram_right_context    419524 non-null float64\n",
      "fivegram_right_context    416502 non-null float64\n",
      "dtypes: float64(22), int64(2), object(1)\n",
      "memory usage: 89.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# check columns\n",
    "fce.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature engineering - analysis and explanation\n",
    "\n",
    "### A. Creating features from language model probabilities\n",
    "\n",
    "Recall that a key hypothesis was that we ngram language model probabilities would indicate whether a word is corrector incorrect given the context. For example, we could build on the intuition that where introducing a new word to a given context causes the ngram probability to hit zero or become very low, it is likely to be incorrect. More specifically, the hypothesis was that a machine learning classifier could learn the decision threshold based on some combination of ngram probabilities.\n",
    "\n",
    "Given that I already have our word match counts and our left / right context counts generated from Google Ngrams, all I need to do is the latter by the former and insert this as a feature column in our dataset. However, in addition, I will take the natural log of all the probabilities. As we'll be dealing with extremely small values, taking the logarithm will help me achieve better simplicity and stability.\n",
    "\n",
    "\n",
    "### Unigram probabilities - naive assumptions\n",
    "\n",
    "In terms of determining the probability for unigram, the standard language model approach would be to take the count of the word in the corpus divided by the the total number of words in the corpus. However, as we're only interested in the *correctness* of the word, I will substitute this for the following naive assumptions: \n",
    "\n",
    "**if the natural logarithm of a word's count is above Q1 - 3 * IQR, we assign the probability to 1**: the assumption is that words appearing not at all or very infrequently in Google Ngrams is very likely to be incorrect. I have decided to use the Tukey method to determine a very low word count as it is more robust to a heavily skewed distribution.\n",
    "\n",
    "**if the natural logarithm of a word's count is below the Tukey threshold**: assign a probability of 0.\n",
    "\n",
    "### Cleaning and imputing probabilities for higher order contexts\n",
    "\n",
    "Another issue I will have to deal with is data sparsity and null values. \n",
    "\n",
    "Depending on their position in a sentence (and the length of that sentences), certain words do not have as much left or right context to work with. For example, \n",
    "- first word: no words before it, therefore *no left context at all*. \n",
    "- second word: only bigram context (i.e. only one word before it)\n",
    "- third word: only bigram and trigram context (i.e. only two words before it)...\n",
    "\n",
    "and so on.\n",
    "\n",
    "- last word: no words after it, so no right context at all\n",
    "- penultimate word: only bigram context (i.e. only one word after it)...\n",
    "\n",
    "and so on.\n",
    "\n",
    "\n",
    "Where words have less context, the best approach is either a) to assign these as null values or b) impute by carrying forward the probability from the highest available ngram context. \n",
    "\n",
    "The reason why I have taken this approach is that these words have fewer syntactic dependencies - the context we have available is the only context we want to take into account for that word. Given that we are only interested in the probability and ultimately *correctness* of a word in *this specific context only*, we should not make assumptions about contexts that don't exist. \n",
    "\n",
    "\n",
    "### B. Creating features from raw match counts\n",
    "\n",
    "Ngram probabilities can be helpful in identifying errors, but they only take into account the ngrams where the word appears either at the beginning or the end of a setnence. Recall that we also have counts for ngrams where the word appears in the middle of the sentence.\n",
    "\n",
    "It might add some additional, useful signal to our model if I include features based on the \"raw\" match counts, which take these other ngrams into account. This would include:\n",
    "\n",
    "- **Mean log match counts for each ngram length**\n",
    "\n",
    "My hypothesis here is that incorrect words will appear in more zero / low count ngrams than a correct word. Therefore, lower ngram count means should be correlated with incorrect words - and some combination of ngram means should provide a classifier with signal to discover the decision boundary. \n",
    "\n",
    "Another benefit of taking the mean is that, to a large extent, it avoids the need for imputing. Although, for very short sentences, there will still be unavoidable null mean values for higher order ngrams (e.g. four and fivegrams).\n",
    "\n",
    "- **Sum of ngrams where the log count is above a certain threshold**\n",
    "\n",
    "There are some potential issues with using ngram count means. The main one is that an incorrect word might nevertheless appear in at least one very common ngram, which would skew the the mean.\n",
    "\n",
    "For example, take the phrase: \"the mistake what I did . \". This is incorrect due to the word \"what\", but the trigram \"what I did\" is so common that it might completely counteract low counts for \"the mistake what\", \"mistake what I\". In fact, it may end up getting a higher mean than a phrase.\n",
    "\n",
    "Taking the logarithm of the counts before calculating the mean might help lessen this issue. However, another way to get around this would be manually creating a feature that creates a binary difference between a significant and insignificant / zero count (as we did with the unigram probabilities). \n",
    "\n",
    "More specifically, for each ngram length, we could sum the number of ngrams where there is an insignificant / zero count.\n",
    "\n",
    "- **Directly using the scaled log counts as features**\n",
    "\n",
    "Another potential option is to simply use the log match counts themselves as features rather than doing any further.\n",
    "\n",
    "The issue here, however, is that we are again faced with sparsity and null values - not all words have the full complement of values.\n",
    "\n",
    "To address this, I could impute by forward filling again. But different length ngrams operate on different scales (i.e. lower-order ngrams have much higher counts than higher-order ones) - imputing would completely bias the data. A better option is to first standard scale the match counts and then forward fill.\n",
    " \n",
    "\n",
    "### C. Creating features using POS tags\n",
    "\n",
    "I will include two features based on the POS tags I previously extracted using Spacy:\n",
    "\n",
    "- **Binary feature: is the word a proper noun or not?** \n",
    "\n",
    "Proper nouns are much less likely to appear in the corpus (especially in longer ngram contexts) than other words. Due to this sparsity issue, the presence of proper nouns could significantly bias my results - causing correct phrases to return a zero count. To try and counter this, I will include in the model a binary feature indicating whether the word is a proper noun.\n",
    "\n",
    "- **Part of speech trigrams** \n",
    "\n",
    "Another useful feature will be to identify whether certain part of speech ngrams are associated with language errors. For example, a trigram of **adjective, adjective, full stop** is very likely to contain an error. \n",
    "\n",
    "The best way to extract such a feature will be to use a Count Vectorizer to map individual words to the ngrams in which they appear. I will take this step in Part 3 of the project.\n",
    "\n",
    "To not overload my model with complexity, I will start by only looking at trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7A. Creating features from language model probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide match count by the left context and create new feature columns\n",
    "count_1 = [\"bigram_left_context\", \"trigram_left_context\", \"fourgram_left_context\", \"fivegram_left_context\"]\n",
    "count_2 = [\"bigram_1\", \"trigram_1\", \"fourgram_1\", \"fivegram_1\"]\n",
    "for i in range(len(count_1)):\n",
    "    fce[\"proba_\" + count_2[i]] = (fce[count_2[i]] / (fce[count_1[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute null ngram match counts and right context match counts by forward filling \n",
    "fce_ff = fce.loc[:, \"bigram_1\": \"fivegram_5\"]\n",
    "fce_ff.loc[:, \"bigram_1\":\"bigram_2\"] = fce_ff.loc[\n",
    "    :, \"bigram_1\":\"bigram_2\"].fillna(method=\"ffill\", axis=1)\n",
    "fce_ff.loc[:, \"trigram_1\":\"trigram_3\"] = fce_ff.loc[\n",
    "    :, \"trigram_1\":\"trigram_3\"].fillna(method=\"ffill\", axis=1)\n",
    "fce_ff.loc[:, \"fourgram_1\":\"fourgram_4\"] = fce_ff.loc[\n",
    "    :, \"fourgram_1\":\"fourgram_4\"].fillna(method=\"ffill\", axis=1)\n",
    "fce_ff.loc[:, \"fivegram_1\":\"fivegram_5\"] = fce_ff.loc[\n",
    "    :, \"fivegram_1\":\"fivegram_5\"].fillna(method=\"ffill\", axis=1)\n",
    "# fce_ff_2 = fce.loc[:, \"bigram_right_context\":\"fivegram_right_context\"].fillna(method=\"ffill\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide match count by the right context and create new feature columns\n",
    "count_1 = [\"bigram_right_context\", \"trigram_right_context\", \"fourgram_right_context\", \"fivegram_right_context\"]\n",
    "count_2 = [\"bigram_2\", \"trigram_3\", \"fourgram_4\", \"fivegram_5\"]\n",
    "for i in range(len(count_1)):\n",
    "    fce[\"proba_\" + count_2[i]] = (fce_ff[count_2[i]] / (fce[count_1[i]]))\n",
    "\n",
    "# delete temporary dataframes\n",
    "del fce_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "fce.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust sentence indices\n",
    "sentence_indices = [[i, j-1] for i, j in sentence_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate outlier threhsolds using tukey method\n",
    "unigram_upper, unigram_lower = ed.tukey_outlier_bounds(np.log(fce[\"unigram_1\"]), 3)\n",
    "bigram_upper, bigram_lower = ed.tukey_outlier_bounds(np.log(fce[\"bigram_1\"]), 3)\n",
    "trigram_upper, trigram_lower = ed.tukey_outlier_bounds(np.log(fce[\"trigram_1\"]), 3)\n",
    "fourgram_upper, fourgram_lower = ed.tukey_outlier_bounds(np.log(fce[\"fourgram_1\"]), 3)\n",
    "fivegram_upper, fivegram_lower = ed.tukey_outlier_bounds(np.log(fce[\"fivegram_1\"]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute probability for unigrams\n",
    "fce[\"proba_word\"] = fce[\"unigram_1\"].map(lambda x: 1 if np.log(x)>unigram_lower else 0)\n",
    "\n",
    "fce.loc[[i[0] for i in sentence_indices], \"proba_bigram_1\"] = fce.loc[\n",
    "    [i[0] for i in sentence_indices], \"unigram_1\"].map(lambda x: 1 if np.log(x)>unigram_lower else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct word probabilities at beginning of sentences by assigning null values \n",
    "fce.loc[[i[0] for i in sentence_indices], \"proba_trigram_1\":\"proba_fivegram_1\"] = np.nan\n",
    "\n",
    "fce.loc[[i[0]+1 if i[0]+1 <= i[1] else i[0] for i in sentence_indices], \n",
    "        \"proba_trigram_1\":\"proba_fivegram_1\"] = np.nan\n",
    "fce.loc[[i[0]+2 if i[0]+2 <= i[1] else i[0] for i in sentence_indices], \n",
    "            \"proba_fourgram_1\":\"proba_fivegram_1\"] = np.nan\n",
    "\n",
    "fce.loc[[i[0]+3 if i[0]+3 <= i[1] else i[0] for i in sentence_indices], \"proba_fivegram_1\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct word probabilities at end of sentences by assigning null values \n",
    "fce.loc[[i[1] for i in sentence_indices], \"proba_bigram_2\"] = fce.loc[\n",
    "    [i[1] for i in sentence_indices], \"unigram_1\"].map(lambda x: 1 if np.log(x)>0 else 0)\n",
    "\n",
    "fce.loc[[i[1] for i in sentence_indices], \"proba_trigram_3\":\"proba_fivegram_5\"] = np.nan\n",
    "fce.loc[[i[1]-1 if i[1]-1 >= i[0] else i[1] for i in sentence_indices], \n",
    "        \"proba_trigram_3\":\"proba_fivegram_5\"] = np.nan\n",
    "fce.loc[[i[1]-2 if i[1]-2 >= i[0] else i[1] for i in sentence_indices], \n",
    "        \"proba_fourgram_4\":\"proba_fivegram_5\"] = np.nan\n",
    "fce.loc[[i[1]-3 if i[1]-3 >= i[0] else i[1] for i in sentence_indices ], \"proba_fivegram_5\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take log of probabilities, setting a sufficiently low negative value (-20) where probability is 0\n",
    "fce.loc[:,\"proba_bigram_1\":\"proba_fivegram_5\"] = fce.loc[\n",
    "    :,\"proba_bigram_1\":\"proba_fivegram_5\"].applymap(lambda x: -20 if (x==0) else np.log(x)) \n",
    "fce[\"proba_word\"] = fce[\"proba_word\"].map(lambda x: -20 if (x==0) else np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7B. Create raw match count based features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take log of counts, first setting any zero counts to value 0 (to avoid issues with log(0))\n",
    "fce.loc[:,\"unigram_1\":\"fivegram_5\"] = fce.loc[\n",
    "    :,\"unigram_1\":\"fivegram_5\"].applymap(lambda x: 0 if (x==0) else np.log(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the log means and place into new columns in the dataframe\n",
    "fce[\"unigram_mean\"] = fce.loc[:, \"unigram_1\"]\n",
    "fce[\"bigram_mean\"] = fce.loc[:, \"bigram_1\" : \"bigram_2\"].mean(axis=1)\n",
    "fce[\"trigram_mean\"] = fce.loc[:, \"trigram_1\" : \"trigram_3\"].mean(axis=1)\n",
    "fce[\"fourgram_mean\"] = fce.loc[:, \"fourgram_1\" : \"fourgram_4\"].mean(axis=1)\n",
    "fce[\"fivegram_mean\"] = fce.loc[:, \"fivegram_1\" : \"fivegram_5\"].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum number of ngrams where value is above lower threshold\n",
    "fce[\"unigram_sum_threshold\"] = fce.loc[:, \"unigram_1\"].map(\n",
    "    lambda x: np.sum(x > unigram_lower))\n",
    "fce[\"bigram_sum_threshold\"] = fce.loc[:, \"bigram_1\" : \"bigram_2\"].apply(\n",
    "    lambda x: np.sum(x > bigram_lower), axis=1)\n",
    "fce[\"trigram_sum_threshold\"] = fce.loc[:, \"trigram_1\" : \"trigram_3\"].apply(\n",
    "    lambda x: np.sum(x > trigram_lower), axis=1)\n",
    "fce[\"fourgram_sum_threshold\"] = fce.loc[:, \"fourgram_1\" : \"fourgram_4\"].apply(\n",
    "    lambda x: np.sum(x > fourgram_lower), axis=1)\n",
    "fce[\"fivegram_sum_threshold\"] = fce.loc[:, \"fivegram_1\" : \"fivegram_5\"].apply(\n",
    "    lambda x: np.sum(x > fivegram_lower), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise and impute match counts - insert them as new feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by manually calculating standard scores (Standard Scalar won't work with null values)\n",
    "fce_ss = ed.manual_zscore(fce.loc[:,\"unigram_1\":\"fivegram_5\"])\n",
    "\n",
    "# forward fill\n",
    "fce_ss = fce_ss.fillna(method='ffill', axis=1)\n",
    "\n",
    "# rename columns\n",
    "fce_ss.columns = [col + \"_scaled\" for col in fce_ss.columns]\n",
    "\n",
    "# concatenate dataframes\n",
    "fce = pd.concat([fce, fce_ss], axis=1)\n",
    "\n",
    "# delete fce_ss\n",
    "del fce_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7C. Create POS tagged features\n",
    "\n",
    "Recall that at the outset, I created a second dataframe (fce_pos) containing the POS tagged ngrams and ngram boundaries.\n",
    "\n",
    "I will now use this dataframe to:\n",
    "- extract a binary \"proper_noun\" binary feature and include within the fce dataframe\n",
    "- insert trigram boundaries within fce dataframe for processing at modelling stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary POS feature\n",
    "fce[\"proper_noun\"] = fce_pos[\"unigrams_1\"].map(lambda x: 1 if \"NNP\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge trigram boundaries\n",
    "fce[\"trigram_boundaries\"] = fce_pos[\"tagged_trigram_boundaries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Check for and clean anomalies and outliers\n",
    "\n",
    "Next, I will check the summary statistics to identify any outliers or anomalies before dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>unigram_1</th>\n",
       "      <th>bigram_1</th>\n",
       "      <th>bigram_2</th>\n",
       "      <th>trigram_1</th>\n",
       "      <th>trigram_2</th>\n",
       "      <th>trigram_3</th>\n",
       "      <th>fourgram_1</th>\n",
       "      <th>fourgram_2</th>\n",
       "      <th>fourgram_3</th>\n",
       "      <th>fourgram_4</th>\n",
       "      <th>fivegram_1</th>\n",
       "      <th>fivegram_2</th>\n",
       "      <th>fivegram_3</th>\n",
       "      <th>fivegram_4</th>\n",
       "      <th>fivegram_5</th>\n",
       "      <th>bigram_left_context</th>\n",
       "      <th>trigram_left_context</th>\n",
       "      <th>fourgram_left_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452569.000000</td>\n",
       "      <td>395635.000000</td>\n",
       "      <td>450437.000000</td>\n",
       "      <td>394216.000000</td>\n",
       "      <td>342252.000000</td>\n",
       "      <td>446180.000000</td>\n",
       "      <td>392600.000000</td>\n",
       "      <td>341465.000000</td>\n",
       "      <td>292691.000000</td>\n",
       "      <td>442948.000000</td>\n",
       "      <td>390239.000000</td>\n",
       "      <td>339869.000000</td>\n",
       "      <td>291737.000000</td>\n",
       "      <td>246467.000000</td>\n",
       "      <td>4.525690e+05</td>\n",
       "      <td>4.504370e+05</td>\n",
       "      <td>4.461800e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.869937</td>\n",
       "      <td>19.682170</td>\n",
       "      <td>14.133499</td>\n",
       "      <td>14.052845</td>\n",
       "      <td>9.291744</td>\n",
       "      <td>9.103233</td>\n",
       "      <td>9.033668</td>\n",
       "      <td>5.193553</td>\n",
       "      <td>4.935890</td>\n",
       "      <td>4.810206</td>\n",
       "      <td>4.778222</td>\n",
       "      <td>2.316988</td>\n",
       "      <td>2.070622</td>\n",
       "      <td>1.929279</td>\n",
       "      <td>1.876632</td>\n",
       "      <td>1.865009</td>\n",
       "      <td>2.932671e+09</td>\n",
       "      <td>5.276966e+07</td>\n",
       "      <td>1.312524e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.336374</td>\n",
       "      <td>3.179168</td>\n",
       "      <td>4.085849</td>\n",
       "      <td>4.090901</td>\n",
       "      <td>4.741899</td>\n",
       "      <td>4.731250</td>\n",
       "      <td>4.741062</td>\n",
       "      <td>4.655509</td>\n",
       "      <td>4.548952</td>\n",
       "      <td>4.518285</td>\n",
       "      <td>4.520664</td>\n",
       "      <td>3.591037</td>\n",
       "      <td>3.383543</td>\n",
       "      <td>3.282896</td>\n",
       "      <td>3.254113</td>\n",
       "      <td>3.239136</td>\n",
       "      <td>5.600165e+09</td>\n",
       "      <td>2.336024e+08</td>\n",
       "      <td>3.424015e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.770366</td>\n",
       "      <td>12.185186</td>\n",
       "      <td>12.134561</td>\n",
       "      <td>6.626718</td>\n",
       "      <td>6.461468</td>\n",
       "      <td>6.371612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.692478e+07</td>\n",
       "      <td>1.892930e+05</td>\n",
       "      <td>7.400000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.147392</td>\n",
       "      <td>14.850568</td>\n",
       "      <td>14.719223</td>\n",
       "      <td>10.150387</td>\n",
       "      <td>9.916626</td>\n",
       "      <td>9.829787</td>\n",
       "      <td>5.552960</td>\n",
       "      <td>5.236442</td>\n",
       "      <td>5.087596</td>\n",
       "      <td>5.023881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.961071e+08</td>\n",
       "      <td>2.637777e+06</td>\n",
       "      <td>2.452550e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.821203</td>\n",
       "      <td>17.030865</td>\n",
       "      <td>16.879635</td>\n",
       "      <td>12.847521</td>\n",
       "      <td>12.624192</td>\n",
       "      <td>12.561429</td>\n",
       "      <td>9.019815</td>\n",
       "      <td>8.672144</td>\n",
       "      <td>8.527342</td>\n",
       "      <td>8.480944</td>\n",
       "      <td>5.099866</td>\n",
       "      <td>4.590044</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>4.174387</td>\n",
       "      <td>4.158883</td>\n",
       "      <td>2.087000e+09</td>\n",
       "      <td>2.292302e+07</td>\n",
       "      <td>3.589680e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   y      unigram_1       bigram_1       bigram_2  \\\n",
       "count  452833.000000  452833.000000  452569.000000  395635.000000   \n",
       "mean        0.869937      19.682170      14.133499      14.052845   \n",
       "std         0.336374       3.179168       4.085849       4.090901   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         1.000000      17.770366      12.185186      12.134561   \n",
       "50%         1.000000      20.147392      14.850568      14.719223   \n",
       "75%         1.000000      21.821203      17.030865      16.879635   \n",
       "max         1.000000      23.816403      23.816403      23.816403   \n",
       "\n",
       "           trigram_1      trigram_2      trigram_3     fourgram_1  \\\n",
       "count  450437.000000  394216.000000  342252.000000  446180.000000   \n",
       "mean        9.291744       9.103233       9.033668       5.193553   \n",
       "std         4.741899       4.731250       4.741062       4.655509   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         6.626718       6.461468       6.371612       0.000000   \n",
       "50%        10.150387       9.916626       9.829787       5.552960   \n",
       "75%        12.847521      12.624192      12.561429       9.019815   \n",
       "max        23.816403      23.816403      23.816403      23.816403   \n",
       "\n",
       "          fourgram_2     fourgram_3     fourgram_4     fivegram_1  \\\n",
       "count  392600.000000  341465.000000  292691.000000  442948.000000   \n",
       "mean        4.935890       4.810206       4.778222       2.316988   \n",
       "std         4.548952       4.518285       4.520664       3.591037   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         5.236442       5.087596       5.023881       0.000000   \n",
       "75%         8.672144       8.527342       8.480944       5.099866   \n",
       "max        23.816403      23.816403      23.816403      23.816403   \n",
       "\n",
       "          fivegram_2     fivegram_3     fivegram_4     fivegram_5  \\\n",
       "count  390239.000000  339869.000000  291737.000000  246467.000000   \n",
       "mean        2.070622       1.929279       1.876632       1.865009   \n",
       "std         3.383543       3.282896       3.254113       3.239136   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         4.590044       4.290459       4.174387       4.158883   \n",
       "max        23.816403      23.816403      23.816403      23.816403   \n",
       "\n",
       "       bigram_left_context  trigram_left_context  fourgram_left_context  \n",
       "count         4.525690e+05          4.504370e+05           4.461800e+05  \n",
       "mean          2.932671e+09          5.276966e+07           1.312524e+06  \n",
       "std           5.600165e+09          2.336024e+08           3.424015e+07  \n",
       "min           0.000000e+00          0.000000e+00           0.000000e+00  \n",
       "25%           4.692478e+07          1.892930e+05           7.400000e+02  \n",
       "50%           4.961071e+08          2.637777e+06           2.452550e+04  \n",
       "75%           2.087000e+09          2.292302e+07           3.589680e+05  \n",
       "max           2.204614e+10          2.204614e+10           2.204614e+10  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check summary statistics\n",
    "fce.iloc[:,0:20].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fivegram_left_context</th>\n",
       "      <th>bigram_right_context</th>\n",
       "      <th>trigram_right_context</th>\n",
       "      <th>fourgram_right_context</th>\n",
       "      <th>fivegram_right_context</th>\n",
       "      <th>proba_bigram_1</th>\n",
       "      <th>proba_trigram_1</th>\n",
       "      <th>proba_fourgram_1</th>\n",
       "      <th>proba_fivegram_1</th>\n",
       "      <th>proba_bigram_2</th>\n",
       "      <th>proba_trigram_3</th>\n",
       "      <th>proba_fourgram_4</th>\n",
       "      <th>proba_fivegram_5</th>\n",
       "      <th>proba_word</th>\n",
       "      <th>unigram_mean</th>\n",
       "      <th>bigram_mean</th>\n",
       "      <th>trigram_mean</th>\n",
       "      <th>fourgram_mean</th>\n",
       "      <th>fivegram_mean</th>\n",
       "      <th>unigram_sum_threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.429480e+05</td>\n",
       "      <td>4.255400e+05</td>\n",
       "      <td>4.235180e+05</td>\n",
       "      <td>4.195240e+05</td>\n",
       "      <td>4.165020e+05</td>\n",
       "      <td>4.510550e+05</td>\n",
       "      <td>3.851480e+05</td>\n",
       "      <td>3.170550e+05</td>\n",
       "      <td>2.021670e+05</td>\n",
       "      <td>4.271930e+05</td>\n",
       "      <td>3.703910e+05</td>\n",
       "      <td>3.288370e+05</td>\n",
       "      <td>2.400690e+05</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452569.000000</td>\n",
       "      <td>450437.000000</td>\n",
       "      <td>446180.000000</td>\n",
       "      <td>442948.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.854888e+05</td>\n",
       "      <td>4.378326e+09</td>\n",
       "      <td>8.659375e+07</td>\n",
       "      <td>1.991162e+06</td>\n",
       "      <td>2.017011e+05</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>-0.094295</td>\n",
       "      <td>19.682170</td>\n",
       "      <td>14.171805</td>\n",
       "      <td>9.298811</td>\n",
       "      <td>5.149432</td>\n",
       "      <td>2.231384</td>\n",
       "      <td>0.995285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.394482e+07</td>\n",
       "      <td>6.734600e+09</td>\n",
       "      <td>3.946595e+08</td>\n",
       "      <td>3.625897e+07</td>\n",
       "      <td>3.500512e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.370043</td>\n",
       "      <td>3.179168</td>\n",
       "      <td>3.448339</td>\n",
       "      <td>3.896302</td>\n",
       "      <td>3.760589</td>\n",
       "      <td>2.812585</td>\n",
       "      <td>0.068502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.010283e+01</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-2.012752e+01</td>\n",
       "      <td>-2.007873e+01</td>\n",
       "      <td>-2.007873e+01</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.089914e+07</td>\n",
       "      <td>2.325740e+05</td>\n",
       "      <td>9.140000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-7.054926e+00</td>\n",
       "      <td>-7.133454e+00</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-8.432718e+00</td>\n",
       "      <td>-9.345668e+00</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.770366</td>\n",
       "      <td>12.461696</td>\n",
       "      <td>6.866836</td>\n",
       "      <td>1.902363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.490000e+02</td>\n",
       "      <td>6.091292e+08</td>\n",
       "      <td>3.215607e+06</td>\n",
       "      <td>2.899100e+04</td>\n",
       "      <td>3.220000e+02</td>\n",
       "      <td>-4.712522e+00</td>\n",
       "      <td>-4.397605e+00</td>\n",
       "      <td>-4.503173e+00</td>\n",
       "      <td>-9.181357e+00</td>\n",
       "      <td>-5.168570e+00</td>\n",
       "      <td>-5.033237e+00</td>\n",
       "      <td>-5.953056e+00</td>\n",
       "      <td>-2.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.147392</td>\n",
       "      <td>14.735230</td>\n",
       "      <td>9.896638</td>\n",
       "      <td>4.953866</td>\n",
       "      <td>1.101878</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.068000e+03</td>\n",
       "      <td>6.051835e+09</td>\n",
       "      <td>2.930974e+07</td>\n",
       "      <td>4.635310e+05</td>\n",
       "      <td>9.366750e+03</td>\n",
       "      <td>-2.642870e+00</td>\n",
       "      <td>-2.452737e+00</td>\n",
       "      <td>-2.171249e+00</td>\n",
       "      <td>-2.256229e+00</td>\n",
       "      <td>-1.706916e+00</td>\n",
       "      <td>-1.238425e+00</td>\n",
       "      <td>-4.274440e-01</td>\n",
       "      <td>-8.396701e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.821203</td>\n",
       "      <td>16.559352</td>\n",
       "      <td>12.175507</td>\n",
       "      <td>8.102350</td>\n",
       "      <td>3.719312</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>23.816403</td>\n",
       "      <td>19.935604</td>\n",
       "      <td>18.099192</td>\n",
       "      <td>22.299355</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fivegram_left_context  bigram_right_context  trigram_right_context  \\\n",
       "count           4.429480e+05          4.255400e+05           4.235180e+05   \n",
       "mean            1.854888e+05          4.378326e+09           8.659375e+07   \n",
       "std             3.394482e+07          6.734600e+09           3.946595e+08   \n",
       "min             0.000000e+00          0.000000e+00           0.000000e+00   \n",
       "25%             0.000000e+00          6.089914e+07           2.325740e+05   \n",
       "50%             2.490000e+02          6.091292e+08           3.215607e+06   \n",
       "75%             8.068000e+03          6.051835e+09           2.930974e+07   \n",
       "max             2.204614e+10          2.204614e+10           2.204614e+10   \n",
       "\n",
       "       fourgram_right_context  fivegram_right_context  proba_bigram_1  \\\n",
       "count            4.195240e+05            4.165020e+05    4.510550e+05   \n",
       "mean             1.991162e+06            2.017011e+05             inf   \n",
       "std              3.625897e+07            3.500512e+07             NaN   \n",
       "min              0.000000e+00            0.000000e+00   -2.010283e+01   \n",
       "25%              9.140000e+02            0.000000e+00   -7.054926e+00   \n",
       "50%              2.899100e+04            3.220000e+02   -4.712522e+00   \n",
       "75%              4.635310e+05            9.366750e+03   -2.642870e+00   \n",
       "max              2.204614e+10            2.204614e+10             inf   \n",
       "\n",
       "       proba_trigram_1  proba_fourgram_1  proba_fivegram_1  proba_bigram_2  \\\n",
       "count     3.851480e+05      3.170550e+05      2.021670e+05    4.271930e+05   \n",
       "mean               inf               inf               inf             inf   \n",
       "std                NaN               NaN               NaN             NaN   \n",
       "min      -2.000000e+01     -2.000000e+01     -2.000000e+01   -2.012752e+01   \n",
       "25%      -7.133454e+00     -2.000000e+01     -2.000000e+01   -8.432718e+00   \n",
       "50%      -4.397605e+00     -4.503173e+00     -9.181357e+00   -5.168570e+00   \n",
       "75%      -2.452737e+00     -2.171249e+00     -2.256229e+00   -1.706916e+00   \n",
       "max                inf               inf               inf             inf   \n",
       "\n",
       "       proba_trigram_3  proba_fourgram_4  proba_fivegram_5     proba_word  \\\n",
       "count     3.703910e+05      3.288370e+05      2.400690e+05  452833.000000   \n",
       "mean               inf               inf               inf      -0.094295   \n",
       "std                NaN               NaN               NaN       1.370043   \n",
       "min      -2.007873e+01     -2.007873e+01     -2.000000e+01     -20.000000   \n",
       "25%      -9.345668e+00     -2.000000e+01     -2.000000e+01       0.000000   \n",
       "50%      -5.033237e+00     -5.953056e+00     -2.000000e+01       0.000000   \n",
       "75%      -1.238425e+00     -4.274440e-01     -8.396701e-01       0.000000   \n",
       "max                inf               inf               inf       0.000000   \n",
       "\n",
       "        unigram_mean    bigram_mean   trigram_mean  fourgram_mean  \\\n",
       "count  452833.000000  452569.000000  450437.000000  446180.000000   \n",
       "mean       19.682170      14.171805       9.298811       5.149432   \n",
       "std         3.179168       3.448339       3.896302       3.760589   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%        17.770366      12.461696       6.866836       1.902363   \n",
       "50%        20.147392      14.735230       9.896638       4.953866   \n",
       "75%        21.821203      16.559352      12.175507       8.102350   \n",
       "max        23.816403      23.816403      19.935604      18.099192   \n",
       "\n",
       "       fivegram_mean  unigram_sum_threshold  \n",
       "count  442948.000000          452833.000000  \n",
       "mean        2.231384               0.995285  \n",
       "std         2.812585               0.068502  \n",
       "min         0.000000               0.000000  \n",
       "25%         0.000000               1.000000  \n",
       "50%         1.101878               1.000000  \n",
       "75%         3.719312               1.000000  \n",
       "max        22.299355               1.000000  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce.iloc[:,20:40].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_sum_threshold</th>\n",
       "      <th>trigram_sum_threshold</th>\n",
       "      <th>fourgram_sum_threshold</th>\n",
       "      <th>fivegram_sum_threshold</th>\n",
       "      <th>unigram_1_scaled</th>\n",
       "      <th>bigram_1_scaled</th>\n",
       "      <th>bigram_2_scaled</th>\n",
       "      <th>trigram_1_scaled</th>\n",
       "      <th>trigram_2_scaled</th>\n",
       "      <th>trigram_3_scaled</th>\n",
       "      <th>fourgram_1_scaled</th>\n",
       "      <th>fourgram_2_scaled</th>\n",
       "      <th>fourgram_3_scaled</th>\n",
       "      <th>fourgram_4_scaled</th>\n",
       "      <th>fivegram_1_scaled</th>\n",
       "      <th>fivegram_2_scaled</th>\n",
       "      <th>fivegram_3_scaled</th>\n",
       "      <th>fivegram_4_scaled</th>\n",
       "      <th>fivegram_5_scaled</th>\n",
       "      <th>proper_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>4.528330e+05</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>452833.000000</td>\n",
       "      <td>425780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.873106</td>\n",
       "      <td>2.621066</td>\n",
       "      <td>3.252713</td>\n",
       "      <td>3.779009</td>\n",
       "      <td>1.927964e-12</td>\n",
       "      <td>-0.000950</td>\n",
       "      <td>0.035036</td>\n",
       "      <td>-0.006889</td>\n",
       "      <td>0.028389</td>\n",
       "      <td>0.061138</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.057679</td>\n",
       "      <td>0.087217</td>\n",
       "      <td>-0.012662</td>\n",
       "      <td>0.020439</td>\n",
       "      <td>0.050783</td>\n",
       "      <td>0.078106</td>\n",
       "      <td>0.102562</td>\n",
       "      <td>0.028919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.334602</td>\n",
       "      <td>0.717913</td>\n",
       "      <td>1.140584</td>\n",
       "      <td>1.563177</td>\n",
       "      <td>1.000001e+00</td>\n",
       "      <td>1.001530</td>\n",
       "      <td>1.007168</td>\n",
       "      <td>1.007349</td>\n",
       "      <td>1.009911</td>\n",
       "      <td>1.011067</td>\n",
       "      <td>1.008266</td>\n",
       "      <td>1.017503</td>\n",
       "      <td>1.025008</td>\n",
       "      <td>1.031344</td>\n",
       "      <td>1.008802</td>\n",
       "      <td>1.029563</td>\n",
       "      <td>1.049353</td>\n",
       "      <td>1.067717</td>\n",
       "      <td>1.084771</td>\n",
       "      <td>0.167578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.190988e+00</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>-6.190988</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-6.013541e-01</td>\n",
       "      <td>-0.478121</td>\n",
       "      <td>-0.442608</td>\n",
       "      <td>-0.572908</td>\n",
       "      <td>-0.535833</td>\n",
       "      <td>-0.502986</td>\n",
       "      <td>-1.115573</td>\n",
       "      <td>-1.085062</td>\n",
       "      <td>-1.064610</td>\n",
       "      <td>-1.056975</td>\n",
       "      <td>-0.645215</td>\n",
       "      <td>-0.611970</td>\n",
       "      <td>-0.587677</td>\n",
       "      <td>-0.576696</td>\n",
       "      <td>-0.576696</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.463349e-01</td>\n",
       "      <td>0.174458</td>\n",
       "      <td>0.204275</td>\n",
       "      <td>0.175923</td>\n",
       "      <td>0.206681</td>\n",
       "      <td>0.240622</td>\n",
       "      <td>0.070434</td>\n",
       "      <td>0.105987</td>\n",
       "      <td>0.145598</td>\n",
       "      <td>0.180924</td>\n",
       "      <td>-0.645215</td>\n",
       "      <td>-0.611970</td>\n",
       "      <td>-0.587677</td>\n",
       "      <td>-0.576696</td>\n",
       "      <td>-0.575775</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.728286e-01</td>\n",
       "      <td>0.708773</td>\n",
       "      <td>0.738919</td>\n",
       "      <td>0.745837</td>\n",
       "      <td>0.781629</td>\n",
       "      <td>0.818093</td>\n",
       "      <td>0.815275</td>\n",
       "      <td>0.856571</td>\n",
       "      <td>0.891985</td>\n",
       "      <td>0.923284</td>\n",
       "      <td>0.762805</td>\n",
       "      <td>0.807852</td>\n",
       "      <td>0.850629</td>\n",
       "      <td>0.891947</td>\n",
       "      <td>0.938606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.300415e+00</td>\n",
       "      <td>2.369866</td>\n",
       "      <td>2.386655</td>\n",
       "      <td>3.063050</td>\n",
       "      <td>3.109789</td>\n",
       "      <td>3.118026</td>\n",
       "      <td>4.000179</td>\n",
       "      <td>4.150525</td>\n",
       "      <td>4.206513</td>\n",
       "      <td>4.211375</td>\n",
       "      <td>5.986973</td>\n",
       "      <td>6.426935</td>\n",
       "      <td>6.667027</td>\n",
       "      <td>6.742179</td>\n",
       "      <td>6.776942</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bigram_sum_threshold  trigram_sum_threshold  fourgram_sum_threshold  \\\n",
       "count         452833.000000          452833.000000           452833.000000   \n",
       "mean               1.873106               2.621066                3.252713   \n",
       "std                0.334602               0.717913                1.140584   \n",
       "min                0.000000               0.000000                0.000000   \n",
       "25%                2.000000               3.000000                3.000000   \n",
       "50%                2.000000               3.000000                4.000000   \n",
       "75%                2.000000               3.000000                4.000000   \n",
       "max                2.000000               3.000000                4.000000   \n",
       "\n",
       "       fivegram_sum_threshold  unigram_1_scaled  bigram_1_scaled  \\\n",
       "count           452833.000000      4.528330e+05    452833.000000   \n",
       "mean                 3.779009      1.927964e-12        -0.000950   \n",
       "std                  1.563177      1.000001e+00         1.001530   \n",
       "min                  0.000000     -6.190988e+00        -6.190988   \n",
       "25%                  3.000000     -6.013541e-01        -0.478121   \n",
       "50%                  5.000000      1.463349e-01         0.174458   \n",
       "75%                  5.000000      6.728286e-01         0.708773   \n",
       "max                  5.000000      1.300415e+00         2.369866   \n",
       "\n",
       "       bigram_2_scaled  trigram_1_scaled  trigram_2_scaled  trigram_3_scaled  \\\n",
       "count    452833.000000     452833.000000     452833.000000     452833.000000   \n",
       "mean          0.035036         -0.006889          0.028389          0.061138   \n",
       "std           1.007168          1.007349          1.009911          1.011067   \n",
       "min          -6.190988         -6.190988         -6.190988         -6.190988   \n",
       "25%          -0.442608         -0.572908         -0.535833         -0.502986   \n",
       "50%           0.204275          0.175923          0.206681          0.240622   \n",
       "75%           0.738919          0.745837          0.781629          0.818093   \n",
       "max           2.386655          3.063050          3.109789          3.118026   \n",
       "\n",
       "       fourgram_1_scaled  fourgram_2_scaled  fourgram_3_scaled  \\\n",
       "count      452833.000000      452833.000000      452833.000000   \n",
       "mean           -0.010184           0.024194           0.057679   \n",
       "std             1.008266           1.017503           1.025008   \n",
       "min            -6.190988          -6.190988          -6.190988   \n",
       "25%            -1.115573          -1.085062          -1.064610   \n",
       "50%             0.070434           0.105987           0.145598   \n",
       "75%             0.815275           0.856571           0.891985   \n",
       "max             4.000179           4.150525           4.206513   \n",
       "\n",
       "       fourgram_4_scaled  fivegram_1_scaled  fivegram_2_scaled  \\\n",
       "count      452833.000000      452833.000000      452833.000000   \n",
       "mean            0.087217          -0.012662           0.020439   \n",
       "std             1.031344           1.008802           1.029563   \n",
       "min            -6.190988          -6.190988          -6.190988   \n",
       "25%            -1.056975          -0.645215          -0.611970   \n",
       "50%             0.180924          -0.645215          -0.611970   \n",
       "75%             0.923284           0.762805           0.807852   \n",
       "max             4.211375           5.986973           6.426935   \n",
       "\n",
       "       fivegram_3_scaled  fivegram_4_scaled  fivegram_5_scaled    proper_noun  \n",
       "count      452833.000000      452833.000000      452833.000000  425780.000000  \n",
       "mean            0.050783           0.078106           0.102562       0.028919  \n",
       "std             1.049353           1.067717           1.084771       0.167578  \n",
       "min            -6.190988          -6.190988          -6.190988       0.000000  \n",
       "25%            -0.587677          -0.576696          -0.576696       0.000000  \n",
       "50%            -0.587677          -0.576696          -0.575775       0.000000  \n",
       "75%             0.850629           0.891947           0.938606       0.000000  \n",
       "max             6.667027           6.742179           6.776942       1.000000  "
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce.iloc[:,40:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be a number of issues:\n",
    "\n",
    "- infinity values in the probabilities: All probabilities should be between 0 and 1 (or as log probabilities, zero or below) yet there appear to be some probabilities with infinite values.\n",
    "- strange maximum values for our raw counts, i.e. all ngram lengths have the same max score. This is highly unexpected and suggestive on an error. We would expect max scores to descend as ngram lengths increase.\n",
    "\n",
    "I will now check some of these in more detail\n",
    "\n",
    "### Check the anomalous maximum values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>fivegram_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236197</th>\n",
       "      <td>me</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word    fivegram_1\n",
       "236197   me  2.204614e+10"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only rows where fivegram match count equals the max value for the column\n",
    "fce.loc[fce[\"fivegram_1\"] == fce[\"fivegram_1\"].max(), [\"word\", \"fivegram_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>fourgram_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236196</th>\n",
       "      <td>write</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word    fourgram_1\n",
       "236196  write  2.204614e+10"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only rows where fourgram match count equals the max value for the column\n",
    "fce.loc[fce[\"fourgram_1\"] == fce[\"fourgram_1\"].max(), [\"word\", \"fourgram_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>trigram_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236195</th>\n",
       "      <td>will</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word     trigram_1\n",
       "236195  will  2.204614e+10"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only rows where trigram match count equals the max value for the column\n",
    "fce.loc[fce[\"trigram_1\"] == fce[\"trigram_1\"].max(), [\"word\", \"trigram_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>bigram_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77880</th>\n",
       "      <td>.</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190032</th>\n",
       "      <td>.</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236194</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word      bigram_1\n",
       "77880     .  2.204614e+10\n",
       "190032    .  2.204614e+10\n",
       "236194    &  2.204614e+10"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only rows where bigram match count equals the max value for the column\n",
    "fce.loc[fce[\"bigram_1\"] == fce[\"bigram_1\"].max(), [\"word\", \"bigram_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly these almost all appear to be in the same sentence. I will look at the surrounding context to try and isolate the cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>fivegram_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236192</th>\n",
       "      <td>forget</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236193</th>\n",
       "      <td>,</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236194</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>7.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236195</th>\n",
       "      <td>will</td>\n",
       "      <td>2.318080e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236196</th>\n",
       "      <td>write</td>\n",
       "      <td>7.832860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236197</th>\n",
       "      <td>me</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236198</th>\n",
       "      <td>soon</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word    fivegram_1\n",
       "236192  forget  0.000000e+00\n",
       "236193       ,  0.000000e+00\n",
       "236194       &  7.200000e+01\n",
       "236195    will  2.318080e+05\n",
       "236196   write  7.832860e+05\n",
       "236197      me  2.204614e+10\n",
       "236198    soon  0.000000e+00"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyse the surrounding context\n",
    "fce.loc[236192:236198, [\"word\", \"fivegram_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem appears to be with the ampersand. I confirmed this after checking how I created the queries in Part 1; Phrasefinder takes percent encoded queries and I failed to replace the ampersands with their percent encoded value.\n",
    "\n",
    "As there are relatively few cases, I will remove these rather than requery them.\n",
    "\n",
    "Finally, I will also check the surrounding context of the other bigram anomalies to see if these are also caused by the ampersand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>bigram_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77875</th>\n",
       "      <td>sit</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77876</th>\n",
       "      <td>down</td>\n",
       "      <td>3.444019e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77877</th>\n",
       "      <td>!</td>\n",
       "      <td>5.165580e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77878</th>\n",
       "      <td>,</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77879</th>\n",
       "      <td>,</td>\n",
       "      <td>7.791411e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77880</th>\n",
       "      <td>.</td>\n",
       "      <td>2.204614e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77881</th>\n",
       "      <td>We</td>\n",
       "      <td>1.830000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77882</th>\n",
       "      <td>ca</td>\n",
       "      <td>1.830000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77883</th>\n",
       "      <td>n't</td>\n",
       "      <td>6.261900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77884</th>\n",
       "      <td>talk</td>\n",
       "      <td>1.952998e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word      bigram_1\n",
       "77875   sit  0.000000e+00\n",
       "77876  down  3.444019e+06\n",
       "77877     !  5.165580e+05\n",
       "77878     ,  0.000000e+00\n",
       "77879     ,  7.791411e+06\n",
       "77880     .  2.204614e+10\n",
       "77881    We  1.830000e+03\n",
       "77882    ca  1.830000e+03\n",
       "77883   n't  6.261900e+04\n",
       "77884  talk  1.952998e+06"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce.loc[77875:77884, [\"word\", \"bigram_1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these cases, it appears to be a sequence of commas causing the problem. Again, checking how I created the queries and with the Phrasefinder API, this has been caused by malformed queries.\n",
    "\n",
    "Again, I will deal with these by removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check anomalies in the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>proba_trigram_1</th>\n",
       "      <th>trigram_1</th>\n",
       "      <th>trigram_left_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12426</th>\n",
       "      <td>.</td>\n",
       "      <td>2.828653</td>\n",
       "      <td>5943.0</td>\n",
       "      <td>2101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17279</th>\n",
       "      <td>is</td>\n",
       "      <td>1.380616</td>\n",
       "      <td>8205.0</td>\n",
       "      <td>5943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28997</th>\n",
       "      <td>and</td>\n",
       "      <td>4.111111</td>\n",
       "      <td>259.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37007</th>\n",
       "      <td>to</td>\n",
       "      <td>21.207765</td>\n",
       "      <td>71555.0</td>\n",
       "      <td>3374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48217</th>\n",
       "      <td>,</td>\n",
       "      <td>5.031008</td>\n",
       "      <td>649.0</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54392</th>\n",
       "      <td>.</td>\n",
       "      <td>2.828653</td>\n",
       "      <td>5943.0</td>\n",
       "      <td>2101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63123</th>\n",
       "      <td>,</td>\n",
       "      <td>16.772794</td>\n",
       "      <td>45991.0</td>\n",
       "      <td>2742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94054</th>\n",
       "      <td>/</td>\n",
       "      <td>222.553207</td>\n",
       "      <td>256424470.0</td>\n",
       "      <td>1152194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94511</th>\n",
       "      <td>,</td>\n",
       "      <td>46.822581</td>\n",
       "      <td>2903.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101603</th>\n",
       "      <td>.</td>\n",
       "      <td>2.828653</td>\n",
       "      <td>5943.0</td>\n",
       "      <td>2101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  proba_trigram_1    trigram_1  trigram_left_context\n",
       "12426     .         2.828653       5943.0                2101.0\n",
       "17279    is         1.380616       8205.0                5943.0\n",
       "28997   and         4.111111        259.0                  63.0\n",
       "37007    to        21.207765      71555.0                3374.0\n",
       "48217     ,         5.031008        649.0                 129.0\n",
       "54392     .         2.828653       5943.0                2101.0\n",
       "63123     ,        16.772794      45991.0                2742.0\n",
       "94054     /       222.553207  256424470.0             1152194.0\n",
       "94511     ,        46.822581       2903.0                  62.0\n",
       "101603    .         2.828653       5943.0                2101.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select first 10 rows where trigrams have a probability greater than 1\n",
    "fce.loc[fce[\"proba_trigram_1\"] > 0, [\"word\", \"proba_trigram_1\", \"trigram_1\", \"trigram_left_context\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it looks like it is punctuation marks and symbols causing the issues due to malformed queries. \n",
    "\n",
    "I'll check the context words to the left to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>proba_trigram_1</th>\n",
       "      <th>trigram_1</th>\n",
       "      <th>trigram_left_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17277</th>\n",
       "      <td>the</td>\n",
       "      <td>0.210194</td>\n",
       "      <td>1239337.0</td>\n",
       "      <td>5896165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17278</th>\n",
       "      <td>U.S.A.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2365910.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17279</th>\n",
       "      <td>is</td>\n",
       "      <td>1.380616</td>\n",
       "      <td>8205.0</td>\n",
       "      <td>5943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17280</th>\n",
       "      <td>July</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17281</th>\n",
       "      <td>due</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17282</th>\n",
       "      <td>to</td>\n",
       "      <td>0.862233</td>\n",
       "      <td>363.0</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17283</th>\n",
       "      <td>the</td>\n",
       "      <td>0.324716</td>\n",
       "      <td>11438013.0</td>\n",
       "      <td>35224703.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  proba_trigram_1   trigram_1  trigram_left_context\n",
       "17277     the         0.210194   1239337.0             5896165.0\n",
       "17278  U.S.A.         0.000000         0.0             2365910.0\n",
       "17279      is         1.380616      8205.0                5943.0\n",
       "17280    July         0.000000         0.0               12403.0\n",
       "17281     due         0.000000         0.0               27080.0\n",
       "17282      to         0.862233       363.0                 421.0\n",
       "17283     the         0.324716  11438013.0            35224703.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce.loc[17277:17283, [\"word\", \"proba_trigram_1\", \"trigram_1\", \"trigram_left_context\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it is the word U.S.A. causing an error due to presence of dots.\n",
    "\n",
    "I will see if this also applies to right contexgt ngrams where probability is greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>proba_bigram_2</th>\n",
       "      <th>bigram_2</th>\n",
       "      <th>bigram_right_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22391</th>\n",
       "      <td>we</td>\n",
       "      <td>98.578632</td>\n",
       "      <td>1.101971e+07</td>\n",
       "      <td>1.117860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31081</th>\n",
       "      <td>they</td>\n",
       "      <td>34.062700</td>\n",
       "      <td>3.807733e+06</td>\n",
       "      <td>1.117860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35573</th>\n",
       "      <td>it</td>\n",
       "      <td>17.389020</td>\n",
       "      <td>1.943849e+06</td>\n",
       "      <td>1.117860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49448</th>\n",
       "      <td>we</td>\n",
       "      <td>91.595562</td>\n",
       "      <td>4.284932e+06</td>\n",
       "      <td>4.678100e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50900</th>\n",
       "      <td>the</td>\n",
       "      <td>19.592211</td>\n",
       "      <td>1.287365e+06</td>\n",
       "      <td>6.570800e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51513</th>\n",
       "      <td>it</td>\n",
       "      <td>17.389020</td>\n",
       "      <td>1.943849e+06</td>\n",
       "      <td>1.117860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73304</th>\n",
       "      <td>we</td>\n",
       "      <td>98.578632</td>\n",
       "      <td>1.101971e+07</td>\n",
       "      <td>1.117860e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77879</th>\n",
       "      <td>,</td>\n",
       "      <td>1.430438</td>\n",
       "      <td>2.204614e+10</td>\n",
       "      <td>1.541216e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94054</th>\n",
       "      <td>/</td>\n",
       "      <td>37.111860</td>\n",
       "      <td>2.564245e+08</td>\n",
       "      <td>6.909502e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>Development</td>\n",
       "      <td>inf</td>\n",
       "      <td>8.526160e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  proba_bigram_2      bigram_2  bigram_right_context\n",
       "22391           we       98.578632  1.101971e+07          1.117860e+05\n",
       "31081         they       34.062700  3.807733e+06          1.117860e+05\n",
       "35573           it       17.389020  1.943849e+06          1.117860e+05\n",
       "49448           we       91.595562  4.284932e+06          4.678100e+04\n",
       "50900          the       19.592211  1.287365e+06          6.570800e+04\n",
       "51513           it       17.389020  1.943849e+06          1.117860e+05\n",
       "73304           we       98.578632  1.101971e+07          1.117860e+05\n",
       "77879            ,        1.430438  2.204614e+10          1.541216e+10\n",
       "94054            /       37.111860  2.564245e+08          6.909502e+06\n",
       "99976  Development             inf  8.526160e+07          0.000000e+00"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only rows where bigram right context probability is greater than 1\n",
    "fce.loc[fce[\"proba_bigram_2\"] > 0, [\"word\", \"proba_bigram_2\", \"bigram_2\", \"bigram_right_context\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>y</th>\n",
       "      <th>unigram_1</th>\n",
       "      <th>bigram_1</th>\n",
       "      <th>bigram_2</th>\n",
       "      <th>trigram_1</th>\n",
       "      <th>trigram_2</th>\n",
       "      <th>trigram_3</th>\n",
       "      <th>fourgram_1</th>\n",
       "      <th>fourgram_2</th>\n",
       "      <th>...</th>\n",
       "      <th>fourgram_right_context</th>\n",
       "      <th>fivegram_right_context</th>\n",
       "      <th>proba_bigram_1</th>\n",
       "      <th>proba_trigram_1</th>\n",
       "      <th>proba_fourgram_1</th>\n",
       "      <th>proba_fivegram_1</th>\n",
       "      <th>proba_bigram_2</th>\n",
       "      <th>proba_trigram_3</th>\n",
       "      <th>proba_fourgram_4</th>\n",
       "      <th>proba_fivegram_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22390</th>\n",
       "      <td>that</td>\n",
       "      <td>1</td>\n",
       "      <td>2997974108</td>\n",
       "      <td>5516337.0</td>\n",
       "      <td>45662178.0</td>\n",
       "      <td>834791.0</td>\n",
       "      <td>144841.0</td>\n",
       "      <td>139063.0</td>\n",
       "      <td>102209.0</td>\n",
       "      <td>24980.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54769.0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>0.129530</td>\n",
       "      <td>0.091773</td>\n",
       "      <td>0.058583</td>\n",
       "      <td>0.088196</td>\n",
       "      <td>5.660246e-02</td>\n",
       "      <td>0.012619</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22391</th>\n",
       "      <td>we</td>\n",
       "      <td>1</td>\n",
       "      <td>806717136</td>\n",
       "      <td>45662178.0</td>\n",
       "      <td>11019711.0</td>\n",
       "      <td>144841.0</td>\n",
       "      <td>139063.0</td>\n",
       "      <td>54769.0</td>\n",
       "      <td>24980.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015231</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.029924</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>9.857863e+01</td>\n",
       "      <td>264.584541</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22392</th>\n",
       "      <td>'ll</td>\n",
       "      <td>1</td>\n",
       "      <td>111786</td>\n",
       "      <td>11019711.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>139063.0</td>\n",
       "      <td>54769.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1545325.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.013767</td>\n",
       "      <td>0.021417</td>\n",
       "      <td>9.228361e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22393</th>\n",
       "      <td>work</td>\n",
       "      <td>1</td>\n",
       "      <td>224308513</td>\n",
       "      <td>207.0</td>\n",
       "      <td>1545325.0</td>\n",
       "      <td>54769.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1545325.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.998092e-02</td>\n",
       "      <td>0.019981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22394</th>\n",
       "      <td>together</td>\n",
       "      <td>1</td>\n",
       "      <td>77340044</td>\n",
       "      <td>1545325.0</td>\n",
       "      <td>77340044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1545325.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.018118e-03</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22395</th>\n",
       "      <td>.</td>\n",
       "      <td>1</td>\n",
       "      <td>15412159799</td>\n",
       "      <td>77340044.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1545325.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  y    unigram_1    bigram_1    bigram_2  trigram_1  trigram_2  \\\n",
       "22390      that  1   2997974108   5516337.0  45662178.0   834791.0   144841.0   \n",
       "22391        we  1    806717136  45662178.0  11019711.0   144841.0   139063.0   \n",
       "22392       'll  1       111786  11019711.0       207.0   139063.0    54769.0   \n",
       "22393      work  1    224308513       207.0   1545325.0    54769.0        0.0   \n",
       "22394  together  1     77340044   1545325.0  77340044.0        0.0  1545325.0   \n",
       "22395         .  1  15412159799  77340044.0         NaN  1545325.0        NaN   \n",
       "\n",
       "       trigram_3  fourgram_1  fourgram_2        ...         \\\n",
       "22390   139063.0    102209.0     24980.0        ...          \n",
       "22391    54769.0     24980.0      1994.0        ...          \n",
       "22392        0.0      1994.0       439.0        ...          \n",
       "22393  1545325.0       439.0      2963.0        ...          \n",
       "22394        NaN      2963.0         0.0        ...          \n",
       "22395        NaN         0.0         NaN        ...          \n",
       "\n",
       "       fourgram_right_context  fivegram_right_context  proba_bigram_1  \\\n",
       "22390                 54769.0                  2963.0        0.129530   \n",
       "22391                     0.0                     0.0        0.015231   \n",
       "22392               1545325.0                     NaN        0.013660   \n",
       "22393                     NaN                     NaN        0.001852   \n",
       "22394                     NaN                     NaN        0.006889   \n",
       "22395                     NaN                     NaN        1.000000   \n",
       "\n",
       "       proba_trigram_1  proba_fourgram_1  proba_fivegram_1  proba_bigram_2  \\\n",
       "22390         0.091773          0.058583          0.088196    5.660246e-02   \n",
       "22391         0.026257          0.029924          0.032052    9.857863e+01   \n",
       "22392         0.003045          0.013767          0.021417    9.228361e-07   \n",
       "22393         0.004970          0.003157          0.000000    1.998092e-02   \n",
       "22394         0.000000          0.054100          0.000000    5.018118e-03   \n",
       "22395         1.000000               NaN          1.000000             NaN   \n",
       "\n",
       "       proba_trigram_3  proba_fourgram_4  proba_fivegram_5  \n",
       "22390         0.012619          0.008015          0.000000  \n",
       "22391       264.584541               inf               inf  \n",
       "22392         0.000000          0.000000          0.001917  \n",
       "22393         0.019981          0.000000               NaN  \n",
       "22394         0.000100               NaN               NaN  \n",
       "22395              NaN               NaN               NaN  \n",
       "\n",
       "[6 rows x 33 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only surrounding context for one of the anamolous words\n",
    "fce.loc[22390:22395]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it is the presence of the \"'ll\" causing the problem. Checking with Phrasefinder, this is because of a parsing issue where some of the Google Ngrams dataset parses \"we'll\", \"they'll\" as one token and others as \"we\" and \"'ll\" as separate tokens.\n",
    "\n",
    "Next, I'll check infinity probabilities and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8197 rows of trigram infinity values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>trigram_3</th>\n",
       "      <th>trigram_right_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>the</td>\n",
       "      <td>7.232733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>the</td>\n",
       "      <td>13.858978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>promised</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>in</td>\n",
       "      <td>11.111686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Capital</td>\n",
       "      <td>12.682984</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>a</td>\n",
       "      <td>16.434304</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>who</td>\n",
       "      <td>8.288283</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>show</td>\n",
       "      <td>11.293600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>saw</td>\n",
       "      <td>7.937375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>last</td>\n",
       "      <td>14.589597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>read</td>\n",
       "      <td>8.519191</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>which</td>\n",
       "      <td>7.684324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>was</td>\n",
       "      <td>10.573980</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>very</td>\n",
       "      <td>11.943272</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>of</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  trigram_3  trigram_right_context\n",
       "70        the   7.232733                    0.0\n",
       "121       the  13.858978                    0.0\n",
       "151  promised        NaN                    0.0\n",
       "227        in  11.111686                    0.0\n",
       "228   Capital  12.682984                    0.0\n",
       "261         a  16.434304                    0.0\n",
       "292       who   8.288283                    0.0\n",
       "446      show  11.293600                    0.0\n",
       "449       saw   7.937375                    0.0\n",
       "450      last  14.589597                    0.0\n",
       "463      read   8.519191                    0.0\n",
       "475     which   7.684324                    0.0\n",
       "493       was  10.573980                    0.0\n",
       "494      very  11.943272                    0.0\n",
       "559        of        NaN                    0.0"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select rows where trigram right context is infinity value\n",
    "print(len(fce.loc[fce[\"proba_trigram_3\"] == np.inf, [\"word\",\"trigram_3\",\"trigram_right_context\"]]),\n",
    "         \"rows of trigram infinity values\")\n",
    "fce.loc[fce[\"proba_trigram_3\"] == np.inf, [\"word\",\"trigram_3\",\"trigram_right_context\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if pandas is assigning some of our 0 probabilities as infinity values. \n",
    "\n",
    "We will have to manually reassign these as 0 probabilities (or log -20, as explained above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean anomalies\n",
    "\n",
    "As stated before, I will deal with the anomalies as follows:\n",
    "- **max count issue:** remove rows\n",
    "- **probabilities greater than 1:** remove rows\n",
    "- **infinity values:** convert to -20.\n",
    "\n",
    "However, it should be noted that when putting the model into production and applying to real data, the parsing / encoding issues that led to the first two issues will have to be corrected. A language checker can't skip words! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change infinity values to -20\n",
    "fce.replace(np.inf, -20, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through probability columns and drop where probability is greater than 0 (log 1)\n",
    "for col in fce.columns[25:33]:\n",
    "    mask = fce[col] > 0\n",
    "    fce = fce.loc[~mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through match count columns and drop where bigrams and larger contexts are equal to unigram max value\n",
    "for col in fce.columns[3:17]:\n",
    "    mask = fce[col] == fce[\"unigram_1\"].max()\n",
    "    fce = fce.loc[~mask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ampersands\n",
    "mask = fce[\"word\"]==\"&\"\n",
    "fce = fce[~mask]\n",
    "mask = fce_pos[\"0\"]==\"&\"\n",
    "fce_pos = fce_pos[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply same cleaning to pos tagged dataframe\n",
    "fce_pos = fce_pos.loc[fce.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save dataframes as CSVs for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "fce.to_csv(\"fce_train_final.csv\")\n",
    "fce_pos.to_csv(\"fce_pos_train_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Repeat process for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FCE test dataset and read in DataFrame\n",
    "my_file = \"fce_test.csv\"\n",
    "fce = pd.read_csv(my_file, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence indices, i.e. indices that mark out the sentences in our dataframe\n",
    "with open(\"sentence_indices_test.pickle\", 'rb') as f:\n",
    "    sentence_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dictionaries of ngram match counts\n",
    "with open('unigram_scores_col_test.pickle','rb') as f:\n",
    "    unigram_scores = pickle.load(f)\n",
    "with open('bigram_scores_col_test.pickle', 'rb') as f:\n",
    "    bigram_scores = pickle.load(f)\n",
    "with open('trigram_scores_col_test.pickle', 'rb') as f:\n",
    "    trigram_scores = pickle.load(f)\n",
    "with open('fourgram_scores_col_test.pickle', 'rb') as f:\n",
    "    fourgram_scores = pickle.load(f)\n",
    "with open('fivegram_scores_col_test.pickle', 'rb') as f:\n",
    "    fivegram_scores = pickle.load(f)\n",
    "\n",
    "# Open dictionaries of ngram context scores\n",
    "with open('bigram_test_context.pickle','rb') as f:\n",
    "    bigram_context = pickle.load(f)\n",
    "with open('trigram_test_context.pickle', 'rb') as f:\n",
    "    trigram_context = pickle.load(f)\n",
    "with open('fourgram_test_context.pickle', 'rb') as f:\n",
    "    fourgram_context = pickle.load(f)\n",
    "with open('fivegram_test_context.pickle', 'rb') as f:\n",
    "    fivegram_context = pickle.load(f)\n",
    "\n",
    "# Open tagged ngrams\n",
    "with open('tagged_unigrams_test.pickle','rb') as f:\n",
    "    tagged_unigrams = pickle.load(f)\n",
    "with open('tagged_bigrams_test.pickle', 'rb') as f:\n",
    "    tagged_bigrams = pickle.load(f)\n",
    "with open('tagged_trigrams_test.pickle', 'rb') as f:\n",
    "    tagged_trigrams = pickle.load(f)\n",
    "with open('tagged_fourgrams_test.pickle', 'rb') as f:\n",
    "    tagged_fourgrams = pickle.load(f)\n",
    "with open('tagged_fivegrams_test.pickle', 'rb') as f:\n",
    "    tagged_fivegrams = pickle.load(f)\n",
    "\n",
    "# Open tagged ngram boundaries\n",
    "with open('tagged_unigrams_boundaries_test.pickle','rb') as f:\n",
    "    tagged_unigrams_boundaries = pickle.load(f)\n",
    "with open('tagged_bigrams_boundaries_test.pickle', 'rb') as f:\n",
    "    tagged_bigrams_boundaries = pickle.load(f)\n",
    "with open('tagged_trigrams_boundaries_test.pickle', 'rb') as f:\n",
    "    tagged_trigrams_boundaries = pickle.load(f)\n",
    "with open('tagged_fourgrams_boundaries_test.pickle', 'rb') as f:\n",
    "    tagged_fourgrams_boundaries = pickle.load(f)\n",
    "with open('tagged_fivegrams_boundaries_test.pickle', 'rb') as f:\n",
    "    tagged_fivegrams_boundaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for each ngram length\n",
    "for key in unigram_scores:\n",
    "    fce[key] = unigram_scores[key]\n",
    "for key in bigram_scores:\n",
    "    fce[key] = bigram_scores[key]\n",
    "for key in trigram_scores:\n",
    "    fce[key] = trigram_scores[key]\n",
    "for key in fourgram_scores:\n",
    "    fce[key] = fourgram_scores[key]\n",
    "for key in fivegram_scores:\n",
    "    fce[key] = fivegram_scores[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column for each ngram left context score\n",
    "fce['bigram_left_context'] = bigram_context['bigram_1_left_context']\n",
    "fce['trigram_left_context'] = trigram_context['trigram_1_left_context']\n",
    "fce['fourgram_left_context'] = fourgram_context['fourgram_1_left_context']\n",
    "fce['fivegram_left_context'] = fivegram_context['fivegram_1_left_context']\n",
    "\n",
    "fce.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# create temporary dataframe with all right context columns\n",
    "fce_rc = pd.DataFrame()\n",
    "\n",
    "for key in bigram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = bigram_context[key]\n",
    "for key in trigram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = trigram_context[key]\n",
    "for key in fourgram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = fourgram_context[key]\n",
    "for key in fivegram_context:\n",
    "    if \"right\" in key:\n",
    "        fce_rc[key] = fivegram_context[key]\n",
    "\n",
    "# replace \"no score\" with np.nan\n",
    "fce_rc.replace(\"No score\", np.nan, inplace=True)\n",
    "\n",
    "# forward fill\n",
    "fce_rc.loc[:, \"bigram_1_right_context\":\"bigram_2_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "fce_rc.loc[:, \"trigram_1_right_context\":\"trigram_3_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "fce_rc.loc[:, \"fourgram_1_right_context\":\"fourgram_4_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "fce_rc.loc[:, \"fivegram_1_right_context\":\"fivegram_5_right_context\"].fillna(method=\"ffill\", axis=1, inplace=True)\n",
    "\n",
    "# create new columns in main dataframe\n",
    "fce['bigram_right_context'] = fce_rc['bigram_2_right_context']\n",
    "fce['trigram_right_context'] = fce_rc['trigram_3_right_context']\n",
    "fce['fourgram_right_context'] = fce_rc['fourgram_4_right_context']\n",
    "fce['fivegram_right_context'] = fce_rc['fivegram_5_right_context']\n",
    "\n",
    "# delete temporary dataframe\n",
    "del fce_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "fce_pos = fce.iloc[:, 0:2]\n",
    "\n",
    "for key in tagged_unigrams:\n",
    "    fce_pos[key] = tagged_unigrams[key]\n",
    "for key in tagged_bigrams:\n",
    "    fce_pos[key] = tagged_bigrams[key]\n",
    "for key in tagged_trigrams:\n",
    "    fce_pos[key] = tagged_trigrams[key]\n",
    "for key in tagged_fourgrams:\n",
    "    fce_pos[key] = tagged_fourgrams[key]\n",
    "for key in tagged_fivegrams:\n",
    "    fce_pos[key] = tagged_fivegrams[key]\n",
    "\n",
    "fce_pos[\"tagged_unigram_boundaries\"] = tagged_unigrams_boundaries\n",
    "fce_pos[\"tagged_bigram_boundaries\"] = tagged_bigrams_boundaries\n",
    "fce_pos[\"tagged_trigram_boundaries\"] = tagged_trigrams_boundaries\n",
    "fce_pos[\"tagged_fourgram_boundaries\"] = tagged_fourgrams_boundaries\n",
    "fce_pos[\"tagged_fivegram_boundaries\"] = tagged_fivegrams_boundaries\n",
    "\n",
    "# rename the first two columns\n",
    "fce.columns = [\"word\", \"y\"] + [col for col in fce.columns[2:]]\n",
    "\n",
    "# Change our outcome variable to a binary variable\n",
    "fce.loc[:,\"y\"] = fce[\"y\"].map(lambda x: 0 if x == \"i\" else 1)\n",
    "\n",
    "# replace \"no score\" with null \n",
    "fce.replace(\"No score\", np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# Divide match count by the left context and create new feature columns\n",
    "count_1 = [\"bigram_left_context\", \"trigram_left_context\", \"fourgram_left_context\", \"fivegram_left_context\"]\n",
    "count_2 = [\"bigram_1\", \"trigram_1\", \"fourgram_1\", \"fivegram_1\"]\n",
    "for i in range(len(count_1)):\n",
    "    fce[\"proba_\" + count_2[i]] = (fce[count_2[i]] / (fce[count_1[i]]))\n",
    "\n",
    "# Impute null ngram match counts and right context match counts by forward filling \n",
    "fce_ff = fce.loc[:, \"bigram_1\": \"fivegram_5\"]\n",
    "fce_ff.loc[:, \"bigram_1\":\"bigram_2\"] = fce_ff.loc[\n",
    "    :, \"bigram_1\":\"bigram_2\"].fillna(method=\"ffill\", axis=1)\n",
    "fce_ff.loc[:, \"trigram_1\":\"trigram_3\"] = fce_ff.loc[\n",
    "    :, \"trigram_1\":\"trigram_3\"].fillna(method=\"ffill\", axis=1)\n",
    "fce_ff.loc[:, \"fourgram_1\":\"fourgram_4\"] = fce_ff.loc[\n",
    "    :, \"fourgram_1\":\"fourgram_4\"].fillna(method=\"ffill\", axis=1)\n",
    "fce_ff.loc[:, \"fivegram_1\":\"fivegram_5\"] = fce_ff.loc[\n",
    "    :, \"fivegram_1\":\"fivegram_5\"].fillna(method=\"ffill\", axis=1)\n",
    "# fce_ff_2 = fce.loc[:, \"bigram_right_context\":\"fivegram_right_context\"].fillna(method=\"ffill\", axis=1)\n",
    "\n",
    "# Divide match count by the right context and create new feature columns\n",
    "count_1 = [\"bigram_right_context\", \"trigram_right_context\", \"fourgram_right_context\", \"fivegram_right_context\"]\n",
    "count_2 = [\"bigram_2\", \"trigram_3\", \"fourgram_4\", \"fivegram_5\"]\n",
    "for i in range(len(count_1)):\n",
    "    fce[\"proba_\" + count_2[i]] = (fce_ff[count_2[i]] / (fce[count_1[i]]))\n",
    "\n",
    "# delete temporary dataframes\n",
    "del fce_ff\n",
    "\n",
    "# reset index\n",
    "fce.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# adjust sentence indices\n",
    "sentence_indices = [[i, j-1] for i, j in sentence_indices]\n",
    "\n",
    "# calculate outlier threhsolds using tukey method\n",
    "unigram_upper, unigram_lower = ed.tukey_outlier_bounds(np.log(fce[\"unigram_1\"]), 3)\n",
    "bigram_upper, bigram_lower = ed.tukey_outlier_bounds(np.log(fce[\"bigram_1\"]), 3)\n",
    "trigram_upper, trigram_lower = ed.tukey_outlier_bounds(np.log(fce[\"trigram_1\"]), 3)\n",
    "fourgram_upper, fourgram_lower = ed.tukey_outlier_bounds(np.log(fce[\"fourgram_1\"]), 3)\n",
    "fivegram_upper, fivegram_lower = ed.tukey_outlier_bounds(np.log(fce[\"fivegram_1\"]), 3)\n",
    "\n",
    "# Impute probability for unigrams\n",
    "fce[\"proba_word\"] = fce[\"unigram_1\"].map(lambda x: 1 if np.log(x)>unigram_lower else 0)\n",
    "\n",
    "fce.loc[[i[0] for i in sentence_indices], \"proba_bigram_1\"] = fce.loc[\n",
    "    [i[0] for i in sentence_indices], \"unigram_1\"].map(lambda x: 1 if np.log(x)>unigram_lower else 0)\n",
    "\n",
    "# Correct word probabilities at beginning of sentences by assigning null values \n",
    "fce.loc[[i[0] for i in sentence_indices], \"proba_trigram_1\":\"proba_fivegram_1\"] = np.nan\n",
    "\n",
    "fce.loc[[i[0]+1 if i[0]+1 <= i[1] else i[0] for i in sentence_indices], \n",
    "        \"proba_trigram_1\":\"proba_fivegram_1\"] = np.nan\n",
    "fce.loc[[i[0]+2 if i[0]+2 <= i[1] else i[0] for i in sentence_indices], \n",
    "            \"proba_fourgram_1\":\"proba_fivegram_1\"] = np.nan\n",
    "\n",
    "fce.loc[[i[0]+3 if i[0]+3 <= i[1] else i[0] for i in sentence_indices], \"proba_fivegram_1\"] = np.nan\n",
    "\n",
    "# Correct word probabilities at end of sentences by assigning null values \n",
    "fce.loc[[i[1] for i in sentence_indices], \"proba_bigram_2\"] = fce.loc[\n",
    "    [i[1] for i in sentence_indices], \"unigram_1\"].map(lambda x: 1 if np.log(x)>0 else 0)\n",
    "\n",
    "fce.loc[[i[1] for i in sentence_indices], \"proba_trigram_3\":\"proba_fivegram_5\"] = np.nan\n",
    "fce.loc[[i[1]-1 if i[1]-1 >= i[0] else i[1] for i in sentence_indices], \n",
    "        \"proba_trigram_3\":\"proba_fivegram_5\"] = np.nan\n",
    "fce.loc[[i[1]-2 if i[1]-2 >= i[0] else i[1] for i in sentence_indices], \n",
    "        \"proba_fourgram_4\":\"proba_fivegram_5\"] = np.nan\n",
    "fce.loc[[i[1]-3 if i[1]-3 >= i[0] else i[1] for i in sentence_indices ], \"proba_fivegram_5\"] = np.nan\n",
    "\n",
    "# take log of probabilities, setting a sufficiently low negative value (-20) where probability is 0\n",
    "fce.loc[:,\"proba_bigram_1\":\"proba_fivegram_5\"] = fce.loc[\n",
    "    :,\"proba_bigram_1\":\"proba_fivegram_5\"].applymap(lambda x: -20 if (x==0) else np.log(x)) \n",
    "fce[\"proba_word\"] = fce[\"proba_word\"].map(lambda x: -20 if (x==0) else np.log(x))\n",
    "\n",
    "# take log of counts, first setting any zero counts to value 0 (to avoid issues with log(0))\n",
    "fce.loc[:,\"unigram_1\":\"fivegram_5\"] = fce.loc[\n",
    "    :,\"unigram_1\":\"fivegram_5\"].applymap(lambda x: 0 if (x==0) else np.log(x)) \n",
    "\n",
    "# Find the log means and place into new columns in the dataframe\n",
    "fce[\"unigram_mean\"] = fce.loc[:, \"unigram_1\"]\n",
    "fce[\"bigram_mean\"] = fce.loc[:, \"bigram_1\" : \"bigram_2\"].mean(axis=1)\n",
    "fce[\"trigram_mean\"] = fce.loc[:, \"trigram_1\" : \"trigram_3\"].mean(axis=1)\n",
    "fce[\"fourgram_mean\"] = fce.loc[:, \"fourgram_1\" : \"fourgram_4\"].mean(axis=1)\n",
    "fce[\"fivegram_mean\"] = fce.loc[:, \"fivegram_1\" : \"fivegram_5\"].mean(axis=1)\n",
    "\n",
    "# Sum number of ngrams where value is above lower threshold\n",
    "fce[\"unigram_sum_threshold\"] = fce.loc[:, \"unigram_1\"].map(\n",
    "    lambda x: np.sum(x > unigram_lower))\n",
    "fce[\"bigram_sum_threshold\"] = fce.loc[:, \"bigram_1\" : \"bigram_2\"].apply(\n",
    "    lambda x: np.sum(x > bigram_lower), axis=1)\n",
    "fce[\"trigram_sum_threshold\"] = fce.loc[:, \"trigram_1\" : \"trigram_3\"].apply(\n",
    "    lambda x: np.sum(x > trigram_lower), axis=1)\n",
    "fce[\"fourgram_sum_threshold\"] = fce.loc[:, \"fourgram_1\" : \"fourgram_4\"].apply(\n",
    "    lambda x: np.sum(x > fourgram_lower), axis=1)\n",
    "fce[\"fivegram_sum_threshold\"] = fce.loc[:, \"fivegram_1\" : \"fivegram_5\"].apply(\n",
    "    lambda x: np.sum(x > fivegram_lower), axis=1)\n",
    "\n",
    "# start by manually calculating standard scores (Standard Scalar won't work with null values)\n",
    "fce_ss = ed.manual_zscore(fce.loc[:,\"unigram_1\":\"fivegram_5\"])\n",
    "\n",
    "# forward fill\n",
    "fce_ss = fce_ss.fillna(method='ffill', axis=1)\n",
    "\n",
    "# rename columns\n",
    "fce_ss.columns = [col + \"_scaled\" for col in fce_ss.columns]\n",
    "\n",
    "# concatenate dataframes\n",
    "fce = pd.concat([fce, fce_ss], axis=1)\n",
    "\n",
    "# delete fce_ss\n",
    "del fce_ss\n",
    "\n",
    "\n",
    "# create binary POS feature\n",
    "fce[\"proper_noun\"] = fce_pos[\"unigrams_1\"].map(lambda x: 1 if \"NNP\" in x else 0)\n",
    "\n",
    "# merge trigram boundaries\n",
    "fce[\"trigram_boundaries\"] = fce_pos[\"tagged_trigram_boundaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change infinity values to -20\n",
    "fce.replace(np.inf, -20, inplace=True)\n",
    "\n",
    "# Loop through probability columns and drop where probability is greater than 0 (log 1)\n",
    "for col in fce.columns[25:33]:\n",
    "    mask = fce[col] > 0\n",
    "    fce = fce.loc[~mask, :]\n",
    "\n",
    "# Loop through match count columns and drop where bigrams and larger contexts are equal to unigram max value\n",
    "for col in fce.columns[3:17]:\n",
    "    mask = fce[col] == fce[\"unigram_1\"].max()\n",
    "    fce = fce.loc[~mask, :]\n",
    "\n",
    "# apply same cleaning to pos tagged dataframe\n",
    "fce_pos = fce_pos.loc[fce.index]\n",
    "\n",
    "# remove ampersands\n",
    "mask = fce[\"word\"]==\"&\"\n",
    "fce = fce[~mask]\n",
    "mask = fce_pos[\"0\"]==\"&\"\n",
    "fce_pos = fce_pos[~mask]\n",
    "\n",
    "fce.to_csv(\"fce_test_final.csv\")\n",
    "fce_pos.to_csv(\"fce_pos_test_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
