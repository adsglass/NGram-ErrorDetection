{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly Data Science Immersive - Capstone Project #\n",
    "\n",
    "## Creating an automated English language error detector ##\n",
    "\n",
    "## Part 1: Data wrangling and API querying - overview of process\n",
    "\n",
    "This is the first part of my data science immersive Capstone Project, covering the initial data wrangling process. I use the Cambridge FCE dataset as the foundation for my project. I generate sets of 1 to 5 overlapping n-gram sets for each word in the dataset and use these to query the Phrasfinder.io API (built on the Google Books Ngrams viewer), which returns a JSON file. \n",
    "\n",
    "From the JSON file I extract the \"match count\" (i.e. the number of appearances the ngram makes in the Google Ngrams dataset) for each ngram that overlaps a given word. I then map these counts back to the original word and its position within the dataframe using Python dictionaries. \n",
    "\n",
    "I also extract the match count of the ngrams to the left and right of the each ngram and map these back to the word. These contextual ngrams counts will be used at a later stage to calculate ngram probabilities, which will be used as features within a classifier.\n",
    "\n",
    "All of these counts are entered as lists into Python dictionaries with each key representing a column in a pandas dataframe. This will allow for easy concatenation with the FCE dataframe at a later stage.\n",
    "\n",
    "Finally, I also parse the sentences using the Spacy NLP library to get the Part of Speech tags, which will also be used in my modelling and feature selection process. \n",
    "\n",
    "As above, these are also entered as lists into Python dictionaries to be later merged into the FCE dataframe.\n",
    "\n",
    "**This entire process is completed twice - once for the training set and once for the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import ErrorDetection as ed\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract sentences from FCE dataset (training set)###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FCE dataset from CSV file\n",
    "my_file = \"./fce-public.train.original.tsv\"\n",
    "fce = pd.read_csv(my_file, header=None, sep='\\t', skip_blank_lines=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>or</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Madam</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0   Dear  c\n",
       "1    Sir  c\n",
       "2     or  c\n",
       "3  Madam  c\n",
       "4      ,  c"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fce.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset is simply divided into words and correct / incorrect (c or i). Each sentence is usefully separated by a row of NaN values, so let's keep those Null value for now.\n",
    "\n",
    "Using a function, I will find the row indices for each sentence and then use these indices to extract the sentences themselves. These sentences will be required later to generate our n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sentence lengths using the indices\n",
    "sentence_length = ed.find_sentence_length(fce, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our sentences and save for later use\n",
    "sentences = ed.create_save_sentences(fce, 0, 'sentences_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our list of sentences looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear Sir or Madam ,',\n",
       " 'I am writing in order to express my disappointment about your musical show \" Over the Rainbow \" .',\n",
       " \"I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends .\",\n",
       " 'I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .',\n",
       " \"The problems started in the box office , where we asked for the discounts you announced in the advertisement , and the man who was selling the tickets said that they did n't exist .\"]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove null values from dataset (training set) and save for later###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the null values having now served their purpose, let's remove them and create / pickle a new set of sentence indices that will work with the non-null dataset. This will come in useful when we need to combine our Google Ngrams data with our original dataframe later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence indices to be used again later with our FCE dataframe without Null values\n",
    "sentence_indices_2 = ed.find_df_sentence_indices(fce, 0, 'sentence_indices_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values from dataframe\n",
    "mask = fce[0].isnull()==True\n",
    "fce = fce[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and delete our dataframe from currnt notebook\n",
    "fce.to_csv(\"fce_train.csv\")\n",
    "del fce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create n-grams from our sentences and run the queries through phrasefinder###\n",
    "\n",
    "The next stage of the process is to:\n",
    "- create a set of overlapping 1-5 length ngrams for each word;\n",
    "- transform these ngrams into a percent encoded format that can be used as queries in Phrasefinder API;\n",
    "- create a *score* dictionary that maps the ngrams (keys) to their queries (values) for future reference;\n",
    "- create a *master reference* dictionary that maps words in the original dataframe to their corresponding ngrams. This master dictionary will enable me to preserve the word order in the dataframe and easily create feature columns at a later stage from the returned Phrasefinder scores.\n",
    "- query Phrasefinder and save (pickle) the resulting JSON file for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28731/28731 [00:03<00:00, 8036.44it/s]\n",
      "100%|██████████| 452833/452833 [00:02<00:00, 213743.44it/s]\n",
      "100%|██████████| 452833/452833 [00:35<00:00, 12872.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# create our trigrams and corresponding queries\n",
    "trigrams_dict, trigrams_reference = ed.create_ngram_dicts(sentences, 3, 'trigram')\n",
    "trigram_queries = ed.create_query_list(trigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query trigrams\n",
    "ed.run_api_queries('trigram', 'train', trigram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28731/28731 [00:04<00:00, 7031.79it/s]\n",
      "100%|██████████| 452833/452833 [00:02<00:00, 163286.62it/s]\n",
      "100%|██████████| 452833/452833 [00:42<00:00, 10600.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# create fourgrams and corresponding queries\n",
    "fourgrams_dict, fourgrams_reference = ed.create_ngram_dicts(sentences, 4, 'fourgram')\n",
    "fourgram_queries = ed.create_query_list(fourgrams_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query fourgrams\n",
    "ed.run_api_queries('fourgram', 'train', fourgram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28731/28731 [00:04<00:00, 6408.00it/s]\n",
      "100%|██████████| 452833/452833 [00:05<00:00, 82811.52it/s] \n",
      "100%|██████████| 452833/452833 [00:50<00:00, 8907.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# create fivegrams and corresponding queries\n",
    "fivegrams_dict, fivegrams_reference = ed.create_ngram_dicts(sentences, 5, 'fivegram')\n",
    "fivegram_queries = ed.create_query_list(fivegrams_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query fivegrams\n",
    "ed.run_api_queries('fivegram', 'train', fivegram_queries, start_value=0, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28731/28731 [00:06<00:00, 4406.19it/s]\n",
      "100%|██████████| 452833/452833 [00:04<00:00, 105643.06it/s]\n",
      "100%|██████████| 452833/452833 [00:35<00:00, 12632.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# create bigrams and corresponding queries\n",
    "bigrams_dict, bigrams_reference = ed.create_ngram_dicts(sentences, 2, 'bigram')\n",
    "bigram_queries = ed.create_query_list(bigrams_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query bigrams\n",
    "ed.run_api_queries('bigram', 'train', bigram_queries, start_value=0, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28731/28731 [00:06<00:00, 4720.33it/s]\n",
      "100%|██████████| 452833/452833 [00:02<00:00, 163671.79it/s]\n",
      "100%|██████████| 452833/452833 [00:13<00:00, 34273.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# create unigrams and corresponding queries\n",
    "unigrams_dict, unigrams_reference = ed.create_ngram_dicts(sentences, 1, 'unigram')\n",
    "unigram_queries = ed.create_query_list(unigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query unigrams\n",
    "ed.run_api_queries('unigram', 'train', unigram_queries, start_value=0, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract the match scores from the phrasefinder JSON files###\n",
    "\n",
    "The JSON files returned by Phrasefinder (and now stored in pickled files) contain the match scores along with other information that I won't need for the purposes of this project. \n",
    "\n",
    "The code below will extract the relevant scores from each of our pickled files and, in a dictionary, map them to their query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_gram_score = ed.load_extract_query_data(\n",
    "    unigram_queries, 'unigram_train', 'unigram_train_final', 1000, \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_score = ed.load_extract_query_data(\n",
    "    bigram_queries, 'bigram_train', 'bigram_train_final', 1000, \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_gram_score = ed.load_extract_query_data(\n",
    "    trigram_queries, 'trigram_train', 'trigram_train_final', 1000, \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_gram_score = ed.load_extract_query_data(\n",
    "    fourgram_queries, 'fourgram_train', 'fourgram_train_final', 1000, \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_gram_score = ed.load_extract_query_data(\n",
    "    fivegram_queries, 'fivegram_train', 'fivegram_train_final', 1000, \"mc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Move scores into our ngram dictionary. Separate out / re-query any errors###\n",
    "\n",
    "I move the scores into the ngram dictionaries I created in step 2, mapping them back to their original ngram. \n",
    "\n",
    "As part of this process, I also capture any errors in the returned JSON files and re-run the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226683/226683 [00:00<00:00, 605041.55it/s]\n",
      "100%|██████████| 290204/290204 [00:00<00:00, 653140.53it/s]\n",
      "100%|██████████| 105564/105564 [00:00<00:00, 641121.53it/s]\n",
      "100%|██████████| 304843/304843 [00:00<00:00, 647105.23it/s]\n",
      "100%|██████████| 14532/14532 [00:00<00:00, 581028.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# move scores into dictionaries and separate out any errors\n",
    "trigrams_dict, trigram_errors = ed.create_score_dict(trigrams_dict, tri_gram_score)\n",
    "fourgrams_dict, fourgram_errors = ed.create_score_dict(fourgrams_dict, four_gram_score)\n",
    "bigrams_dict, bigram_errors = ed.create_score_dict(bigrams_dict, bi_gram_score)\n",
    "fivegrams_dict, fivegram_errors = ed.create_score_dict(fivegrams_dict, five_gram_score)\n",
    "unigrams_dict, unigram_errors = ed.create_score_dict(unigrams_dict, uni_gram_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create a dictionary of scores to be used as columns in the FCE dataframe ###\n",
    "\n",
    "Recall that the ultimate aim of this initial data wrangling process is to get the ngram scores into a format that can be merged with the original FCE dataframe. The final step for achieving this is to create a dictionary of lists where each key represents a proposed column name and each value represents a list of scores.\n",
    "\n",
    "Each key/column will be an ngram that overlaps, e.g. \"five_gram_1\" (the first fivegram that overlaps the word), \"five_gram_2\" (the second fivegram)... \"four_gram_1\"... \"uni_gram_1\" (the number of times the word itself appears in the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a wordscores to the master ngram reference dictionary for future merging with dataframe\n",
    "bigram_wordscore = ed.assign_word_scores(bigrams_reference, bigrams_dict, 'bigram', 'train')\n",
    "trigram_wordscore = ed.assign_word_scores(trigrams_reference, trigrams_dict, 'trigram','train')\n",
    "fourgram_wordscore = ed.assign_word_scores(fourgrams_reference, fourgrams_dict, 'fourgram','train')\n",
    "fivegram_wordscore = ed.assign_word_scores(fivegrams_reference, fivegrams_dict, 'fivegram','train')\n",
    "unigram_wordscore = ed.assign_word_scores(unigrams_reference, unigrams_dict, 'unigram','train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find ngram context scores and create further dictionaries as in step 5 ###\n",
    "\n",
    "As mentioned previously, one of the key features of my model will be the language model probabilities of each ngram containing the word, *given its left and right context*.\n",
    "\n",
    "To give an example, let's take the sentence \"The quick, brown fox jumped over the lazy dog\". For the word \"jumped\", I'd be looking to calculate firstly the *left context* ngram probabilities: \n",
    "- the bigram \"fox jumped\" given the unigram \"fox\"; \n",
    "- the trigram \"brown fox jumped\" given the bigram \"brown fox\"\n",
    "- the fourgram \", brown fox jumped\" given the trigram \"brown fox jumped\"\n",
    "- the fivegram \"quick , brown fox jumped\" given the fourgram \", brown fox jumped\"\n",
    "\n",
    "and secondly, the *right context* ngram probabilities:\n",
    "- the bigram \"jumped over\" given the unigram \"over\"; \n",
    "- the trigram \"jumped over the\" given the bigram \"over the\"\n",
    "- the fourgram \"jumped over the lazy\" given the trigram \"over the lazy\"\n",
    "- the fivegram \"jumped over the lazy dog\" given the fourgram \"over the lazy dog\".\n",
    "\n",
    "So along with the ngram scores I mapped across in step 5, I will also map across the left and right context scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom functions to assign context scores and map across to master dictionary\n",
    "bigram_context_wordscore = ed.assign_context_word_scores(\n",
    "    bigrams_reference, unigrams_dict, 'bigram', 'train')\n",
    "trigram_context_wordscore = ed.assign_context_word_scores(\n",
    "    trigrams_reference, bigrams_dict, 'trigram', 'train')\n",
    "fourgram_context_wordscore = ed.assign_context_word_scores(\n",
    "    fourgrams_reference, trigrams_dict, 'fourgram', 'train')\n",
    "fivegram_context_wordscore = ed.assign_context_word_scores(\n",
    "    fivegrams_reference, fourgrams_dict, 'fivegram', 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Parse sentences using Spacy and extract Part of Speech tags ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Spacy object\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28731it [01:38, 291.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# use multithreading to efficiently parse all sentences\n",
    "parsed_sentences = ed.parse(sentences, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28731/28731 [00:04<00:00, 6127.51it/s]\n",
      "100%|██████████| 452833/452833 [00:01<00:00, 288083.55it/s]\n",
      "100%|██████████| 28731/28731 [00:02<00:00, 13661.96it/s]\n",
      "100%|██████████| 452833/452833 [00:02<00:00, 225886.32it/s]\n",
      "100%|██████████| 28731/28731 [00:02<00:00, 12740.43it/s]\n",
      "100%|██████████| 452833/452833 [00:02<00:00, 184327.43it/s]\n",
      "100%|██████████| 28731/28731 [00:02<00:00, 12261.36it/s]\n",
      "100%|██████████| 452833/452833 [00:02<00:00, 167215.96it/s]\n",
      "100%|██████████| 28731/28731 [00:01<00:00, 22827.26it/s]\n",
      "100%|██████████| 452833/452833 [00:01<00:00, 287099.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# create part of speech tagged ngrams dictionaries and save / pickle them\n",
    "tagged_unigrams, tagged_unigram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 1, 'unigrams', 'train')\n",
    "tagged_bigrams, tagged_bigram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 2, 'bigrams', 'train')\n",
    "tagged_trigrams, tagged_trigram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 3, 'trigrams', 'train')\n",
    "tagged_fourgrams, tagged_fourgram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 4, 'fourgrams', 'train')\n",
    "tagged_fivegrams, tagged_fivegram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 5, 'fivegrams', 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Repeat process for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FCE dataset from CSV file\n",
    "my_file = \"./fce-public.test.original.tsv\"\n",
    "fce = pd.read_csv(my_file, header=None, sep='\\t', skip_blank_lines=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the sentence lengths using the indices\n",
    "sentence_length = ed.find_sentence_length(fce, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our sentences and save for later use\n",
    "sentences = ed.create_save_sentences(fce, 0, 'sentences_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentence indices to be used again later with our FCE dataframe without Null values\n",
    "sentence_indices_2 = ed.find_df_sentence_indices(fce, 0, 'sentence_indices_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null values from dataframe\n",
    "mask = fce[0].isnull()==True\n",
    "fce = fce[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and delete our dataframe from currnt notebook\n",
    "fce.to_csv(\"fce_test.csv\")\n",
    "del fce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2720/2720 [00:00<00:00, 9320.81it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 210760.18it/s]\n",
      "100%|██████████| 41477/41477 [00:03<00:00, 13250.49it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 9866.61it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 260337.89it/s]\n",
      "100%|██████████| 41477/41477 [00:03<00:00, 10809.60it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 9486.71it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 239754.61it/s]\n",
      "100%|██████████| 41477/41477 [00:04<00:00, 9177.16it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 9106.71it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 369413.47it/s]\n",
      "100%|██████████| 41477/41477 [00:02<00:00, 17891.94it/s]\n",
      "100%|██████████| 2720/2720 [00:01<00:00, 2188.62it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 535891.97it/s]\n",
      "100%|██████████| 41477/41477 [00:01<00:00, 32870.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# create ngrams and corresponding queries\n",
    "trigrams_dict, trigrams_reference = ed.create_ngram_dicts(sentences, 3, 'trigram')\n",
    "trigram_queries = ed.create_query_list(trigrams_dict)\n",
    "fourgrams_dict, fourgrams_reference = ed.create_ngram_dicts(sentences, 4, 'fourgram')\n",
    "fourgram_queries = ed.create_query_list(fourgrams_dict)\n",
    "fivegrams_dict, fivegrams_reference = ed.create_ngram_dicts(sentences, 5, 'fivegram')\n",
    "fivegram_queries = ed.create_query_list(fivegrams_dict)\n",
    "bigrams_dict, bigrams_reference = ed.create_ngram_dicts(sentences, 2, 'bigram')\n",
    "bigram_queries = ed.create_query_list(bigrams_dict)\n",
    "unigrams_dict, unigrams_reference = ed.create_ngram_dicts(sentences, 1, 'unigram')\n",
    "unigram_queries = ed.create_query_list(unigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Phrasefinder\n",
    "ed.run_api_queries('trigram', 'test', trigram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Phrasefinder\n",
    "ed.run_api_queries('fourgram', 'test', fourgram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Phrasefinder\n",
    "ed.run_api_queries('fivegram', 'test', fivegram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Phrasefinder\n",
    "ed.run_api_queries('bigram', 'test', bigram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Phrasefinder\n",
    "ed.run_api_queries('unigram', 'test', unigram_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract match scores from JSON files and map across to a dictionary\n",
    "uni_gram_score = ed.load_extract_query_data(\n",
    "    unigram_queries, 'unigram_test', 'unigram_test_final', 1000, \"mc\")\n",
    "\n",
    "bi_gram_score = ed.load_extract_query_data(\n",
    "    bigram_queries, 'bigram_test', 'bigram_test_final', 1000, \"mc\")\n",
    "\n",
    "tri_gram_score = ed.load_extract_query_data(\n",
    "    trigram_queries, 'trigram_test', 'trigram_test_final', 1000, \"mc\")\n",
    "\n",
    "four_gram_score = ed.load_extract_query_data(\n",
    "    fourgram_queries, 'fourgram_test', 'fourgram_test_final', 1000, \"mc\")\n",
    "\n",
    "five_gram_score = ed.load_extract_query_data(\n",
    "    fivegram_queries, 'fivegram_test', 'fivegram_test_final', 1000, \"mc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27558/27558 [00:00<00:00, 397239.04it/s]\n",
      "100%|██████████| 30170/30170 [00:00<00:00, 506795.01it/s]\n",
      "100%|██████████| 17656/17656 [00:00<00:00, 527345.72it/s]\n",
      "100%|██████████| 29683/29683 [00:00<00:00, 549717.75it/s]\n",
      "100%|██████████| 3871/3871 [00:00<00:00, 433229.74it/s]\n",
      "100%|██████████| 30170/30170 [00:00<00:00, 887635.13it/s]\n",
      "100%|██████████| 29683/29683 [00:00<00:00, 1250962.35it/s]\n",
      "100%|██████████| 17656/17656 [00:00<00:00, 791503.30it/s]\n",
      "100%|██████████| 3871/3871 [00:00<00:00, 950594.31it/s]\n",
      "100%|██████████| 27558/27558 [00:00<00:00, 974534.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# move scores into dictionaries and identify / separate out any errors\n",
    "trigrams_dict, trigram_errors = ed.create_score_dict(trigrams_dict, tri_gram_score)\n",
    "fourgrams_dict, fourgram_errors = ed.create_score_dict(fourgrams_dict, four_gram_score)\n",
    "bigrams_dict, bigram_errors = ed.create_score_dict(bigrams_dict, bi_gram_score)\n",
    "fivegrams_dict, fivegram_errors = ed.create_score_dict(fivegrams_dict, five_gram_score)\n",
    "unigrams_dict, unigram_errors = ed.create_score_dict(unigrams_dict, uni_gram_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a wordscores to the master ngram reference dictionary for future merging with dataframe\n",
    "bigram_wordscore = ed.assign_word_scores(bigrams_reference, bigrams_dict, 'bigram', 'test')\n",
    "trigram_wordscore = ed.assign_word_scores(trigrams_reference, trigrams_dict, 'trigram','test')\n",
    "fourgram_wordscore = ed.assign_word_scores(fourgrams_reference, fourgrams_dict, 'fourgram','test')\n",
    "fivegram_wordscore = ed.assign_word_scores(fivegrams_reference, fivegrams_dict, 'fivegram','test')\n",
    "unigram_wordscore = ed.assign_word_scores(unigrams_reference, unigrams_dict, 'unigram','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom functions to assign context scores for future probability calculations\n",
    "# and map across to master dictionary\n",
    "bigram_context_wordscore = ed.assign_context_word_scores(\n",
    "    bigrams_reference, unigrams_dict, 'bigram', 'test')\n",
    "trigram_context_wordscore = ed.assign_context_word_scores(\n",
    "    trigrams_reference, bigrams_dict, 'trigram', 'test')\n",
    "fourgram_context_wordscore = ed.assign_context_word_scores(\n",
    "    fourgrams_reference, trigrams_dict, 'fourgram', 'test')\n",
    "fivegram_context_wordscore = ed.assign_context_word_scores(\n",
    "    fivegrams_reference, fourgrams_dict, 'fivegram', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Spacy object\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2720it [00:08, 314.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# use multithreading to efficiently parse all sentences\n",
    "parsed_sentences = ed.parse(sentences, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2720/2720 [00:00<00:00, 18257.47it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 539118.74it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 23420.85it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 482999.23it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 22087.25it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 321806.20it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 20494.79it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 264469.35it/s]\n",
      "100%|██████████| 2720/2720 [00:00<00:00, 25883.83it/s]\n",
      "100%|██████████| 41477/41477 [00:00<00:00, 217261.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# create part of speech tagged ngrams dictionaries and save / pickle them\n",
    "tagged_unigrams, tagged_unigram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 1, 'unigrams', 'test')\n",
    "tagged_bigrams, tagged_bigram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 2, 'bigrams', 'test')\n",
    "tagged_trigrams, tagged_trigram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 3, 'trigrams', 'test')\n",
    "tagged_fourgrams, tagged_fourgram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 4, 'fourgrams', 'test')\n",
    "tagged_fivegrams, tagged_fivegram_boundaries = ed.create_pos_ngram_dicts(\n",
    "    sentences, parsed_sentences, 5, 'fivegrams', 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
